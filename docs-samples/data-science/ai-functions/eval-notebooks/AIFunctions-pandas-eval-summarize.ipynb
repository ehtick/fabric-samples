{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate ai.summarize(...) Quality\n",
    "\n",
    "This notebook shows how to evaluate the output quality of AI Function `ai.summarize` using **LLM-as-a-Judge** - a technique where a large language model evaluates quality without manually labeled ground truth. This starter notebook uses sample data; replace it with your own data and adapt the eval prompts and criteria as needed.\n",
    "\n",
    "### What You'll Do\n",
    "1. Generate summaries from sample documents using `ai.summarize` defaults\n",
    "2. Use a judge model to score each summary on five quality metrics\n",
    "3. Visualize results with radar and bar charts\n",
    "4. Identify samples that need review\n",
    "5. (Optional) Refine summaries with the `instructions` parameter\n",
    "\n",
    "### Before You Start\n",
    "- **Other AI functions?** Find evaluation notebooks for all AI functions at [aka.ms/fabric-aifunctions-eval-notebooks](https://aka.ms/fabric-aifunctions-eval-notebooks)\n",
    "- **Runtime** - This notebook was made for **Fabric 1.3 runtime**.\n",
    "- **Customize this notebook** - The prompts and evaluation criteria below are a starting point. Adjust them to match your specific use case and quality standards.\n",
    "\n",
    "| Metric | Measures |\n",
    "|--------|----------|\n",
    "| **Fluency** | Grammar & readability |\n",
    "| **Coherence** | Logical flow |\n",
    "| **Conciseness** | Brevity without loss |\n",
    "| **Consistency** | No hallucinations |\n",
    "| **Relevance** | Key info captured |\n",
    "\n",
    "[ai.summarize pandas Documentation](https://learn.microsoft.com/en-us/fabric/data-science/ai-functions/pandas/summarize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "In Fabric 1.3 runtime, pandas AI functions require the openai-python package.\n",
    "\n",
    "See [install instructions for AI Functions](https://learn.microsoft.com/en-us/fabric/data-science/ai-functions/overview?tabs=pandas-pyspark%2Cpandas#install-dependencies) for up-to-date information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q openai 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synapse.ml.aifunc as aifunc\n",
    "from pydantic import BaseModel, Field\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# Executor: runs AI functions on your data\n",
    "executor_conf = aifunc.Conf(\n",
    "    model_deployment_name=\"gpt-4.1-mini\",  # see https://aka.ms/fabric-ai-models for other models\n",
    "    reasoning_effort=None,\n",
    "    verbosity=None,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# Judge: evaluates outputs (use a large frontier model with reasoning for best pseudo ground truth)\n",
    "judge_conf = aifunc.Conf(\n",
    "    model_deployment_name=\"gpt-5\",  # see https://aka.ms/fabric-ai-models for other models\n",
    "    reasoning_effort=\"low\",\n",
    "    verbosity=\"low\",\n",
    "    temperature=None,  # reasoning models only support None, openai.NOT_GIVEN or default value of temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Your Data\n",
    "\n",
    "Replace the sample data below with your own documents. We use two datasets:\n",
    "- **Articles** - long-form text for single-column summarization\n",
    "- **Support tickets** - multi-column structured data for DataFrame summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"article\": [\n",
    "        \"\"\"The Federal Reserve held interest rates steady at its January meeting, keeping \\\n",
    "the benchmark federal funds rate in the 5.25% to 5.50% range for the fourth \\\n",
    "consecutive session. Fed Chair Jerome Powell said during the post-meeting press \\\n",
    "conference that while inflation has moved closer to the central bank's 2% target, \\\n",
    "policymakers want to see \"more evidence\" before beginning to cut rates. The \\\n",
    "consumer price index rose 3.1% year-over-year in December, down from a peak of \\\n",
    "9.1% in June 2022 but still above the Fed's goal. Labor markets remain resilient, \\\n",
    "with the economy adding 216,000 jobs in December and the unemployment rate holding \\\n",
    "at 3.7%. Powell emphasized that the committee does not expect it will be \\\n",
    "appropriate to reduce rates until it has \"greater confidence that inflation is \\\n",
    "moving sustainably toward 2 percent.\" Markets had priced in a roughly 50% chance \\\n",
    "of a rate cut by March, but Powell's comments pushed those expectations out to May \\\n",
    "or June. Treasury yields ticked higher following the announcement, with the \\\n",
    "10-year note rising 5 basis points to 4.07%. Stock futures initially dipped on the \\\n",
    "news but recovered by end of trading as investors digested the overall dovish tone \\\n",
    "of the statement, which removed language about potential further tightening.\"\"\",\n",
    "\n",
    "        \"\"\"Researchers at Stanford University and the Allen Institute for AI have \\\n",
    "published a landmark study demonstrating that large language models trained on \\\n",
    "synthetic data can match or exceed the performance of models trained on human-curated \\\n",
    "datasets across a range of natural language processing benchmarks. The study, \\\n",
    "published in Nature Machine Intelligence, evaluated a family of models called \\\n",
    "SynthLM ranging from 1.3 billion to 70 billion parameters. The researchers generated \\\n",
    "training data by prompting an existing frontier model to produce question-answer \\\n",
    "pairs, reasoning chains, and summarization examples, then filtered outputs for \\\n",
    "quality using a combination of automated checks and a small set of human reviewers. \\\n",
    "On the MMLU benchmark, the 70B SynthLM model scored 84.2%, compared to 83.7% for a \\\n",
    "model of the same size trained on the Pile dataset. On summarization tasks using the \\\n",
    "CNN/DailyMail dataset, SynthLM achieved a ROUGE-L score of 42.8 versus 41.5 for the \\\n",
    "baseline. The researchers noted that synthetic data generation cost approximately \\\n",
    "$2.3 million, compared to an estimated $15 million for curating an equivalent volume \\\n",
    "of human-labeled data. However, Dr. Maria Chen, the lead author, cautioned that \\\n",
    "\"synthetic data amplifies any biases present in the source model\" and recommended \\\n",
    "hybrid approaches that combine synthetic and human-curated data for safety-critical \\\n",
    "applications. The team has released SynthLM-7B under an open license for further \\\n",
    "research.\"\"\",\n",
    "\n",
    "        \"\"\"Bristol-Myers Squibb announced positive results from its Phase III clinical \\\n",
    "trial of mavacamten in patients with obstructive hypertrophic cardiomyopathy, a \\\n",
    "condition affecting roughly 1 in 500 people in which the heart muscle becomes \\\n",
    "abnormally thick and can obstruct blood flow. The VALOR-HCM trial enrolled 112 \\\n",
    "patients across 34 clinical sites in the United States and Europe who were already \\\n",
    "being considered for septal reduction therapy - an invasive procedure to thin the \\\n",
    "heart muscle. After 16 weeks of treatment, only 18% of patients in the mavacamten \\\n",
    "group still met the guideline criteria for the surgical procedure, compared to 77% \\\n",
    "in the placebo group. Patients receiving mavacamten also showed significant \\\n",
    "improvements in exercise capacity, with a mean increase of 2.8 mL/kg/min in peak \\\n",
    "oxygen consumption compared to 0.3 mL/kg/min in the placebo arm. Quality of life \\\n",
    "scores measured by the Kansas City Cardiomyopathy Questionnaire improved by an \\\n",
    "average of 9.1 points in the treatment group versus 1.8 points for placebo. Dr. \\\n",
    "Jonathan Ho, the principal investigator at Massachusetts General Hospital, stated \\\n",
    "that \"these results confirm mavacamten's potential to fundamentally change how we \\\n",
    "treat patients with obstructive HCM, offering a non-invasive alternative to surgery.\" \\\n",
    "Common side effects included dizziness (15%), fatigue (12%), and atrial fibrillation \\\n",
    "(6%). Bristol-Myers Squibb plans to submit the data to the FDA for label expansion \\\n",
    "by mid-2025.\"\"\",\n",
    "\n",
    "        \"\"\"The city of Austin, Texas approved a $7.1 billion transit expansion plan on \\\n",
    "Tuesday that will bring two new light rail lines, 30 miles of dedicated bus rapid \\\n",
    "transit lanes, and a downtown tunnel connecting the city's east and west corridors. \\\n",
    "The plan, known as Project Connect Phase 2, passed with 58% voter approval after a \\\n",
    "contentious campaign that pitted transit advocates against opponents who argued the \\\n",
    "costs would strain the city's budget. Construction on the first light rail segment - \\\n",
    "a 12.4-mile Orange Line running from the Austin-Bergstrom International Airport \\\n",
    "through downtown to the North Lamar Transit Center - is expected to begin in 2026 \\\n",
    "with service starting in 2031. The second Blue Line will run east-west from the \\\n",
    "growing Mueller neighborhood through the University of Texas campus to the Westgate \\\n",
    "shopping district. Capital Metro CEO Randy Clarke said the system is projected to \\\n",
    "carry 87,000 daily riders by 2040, reducing car trips on the I-35 corridor by an \\\n",
    "estimated 14%. The project will be funded through a combination of federal grants \\\n",
    "(35%), a property tax increase of 8.75 cents per $100 of assessed valuation (40%), \\\n",
    "and revenue bonds (25%). Critics, including the Austin Taxpayers Association, have \\\n",
    "argued that the cost-per-mile of $275 million for the light rail exceeds comparable \\\n",
    "projects in Denver and Portland and that ridership projections are overly optimistic \\\n",
    "given the city's sprawling geography.\"\"\",\n",
    "\n",
    "        \"\"\"Patagonia announced a sweeping overhaul of its supply chain on Wednesday, \\\n",
    "committing to sourcing 100% of its cotton from regenerative organic farms by 2030 and \\\n",
    "transitioning all polyester products to recycled or bio-based materials by 2028. The \\\n",
    "outdoor apparel company said it will invest $340 million over five years to support \\\n",
    "the transition, including $120 million in direct grants to farming cooperatives in \\\n",
    "India, Peru, and the United States that adopt regenerative practices such as cover \\\n",
    "cropping, reduced tillage, and integrated pest management. Patagonia currently \\\n",
    "sources about 34% of its cotton from regenerative farms, up from less than 2% in \\\n",
    "2018. CEO Ryan Gellert said in a statement that \"the climate crisis requires us to \\\n",
    "move faster and invest more heavily in the solutions we know work. Regenerative \\\n",
    "agriculture is not just about reducing harm - it actively restores soil health and \\\n",
    "sequesters carbon.\" An independent lifecycle analysis conducted by the consulting \\\n",
    "firm Quantis estimated that full adoption of regenerative cotton would reduce \\\n",
    "Patagonia's Scope 3 emissions by approximately 18%, or 46,000 metric tons of CO2 \\\n",
    "equivalent per year. The company also announced it will publish a quarterly supply \\\n",
    "chain transparency report starting in Q1 2025, disclosing factory-level audit results, \\\n",
    "worker wage data, and environmental impact metrics for each of its 72 Tier 1 suppliers.\"\"\"\n",
    "    ]\n",
    "})\n",
    "print(f\"Loaded {len(df)} articles\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Summaries with Default Settings\n",
    "\n",
    "`ai.summarize` works out of the box - just call it on a column. Here we use `executor_conf` to keep execution settings explicit and consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize using defaults for function behavior (with executor_conf)\n",
    "df[\"summary\"] = df[\"article\"].ai.summarize(conf=executor_conf)\n",
    "\n",
    "# Compare article and summary lengths\n",
    "df[\"word_count\"] = df[\"article\"].astype(str).str.split().str.len()\n",
    "df[\"summary_word_count\"] = df[\"summary\"].astype(str).str.split().str.len()\n",
    "\n",
    "display(df[[\"article\", \"summary\", \"word_count\", \"summary_word_count\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Evaluation Metrics\n",
    "\n",
    "Each summary is scored on 5 metrics (1-5 scale) using G-Eval methodology.\n",
    "\n",
    "> **TIP: XML-formatted prompts** - The evaluation prompts use XML tags like `<evaluation_criteria>` and `<source_text>` to help LLMs distinguish between instructions and data. This improves accuracy. Try this pattern in your own prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"reason\" is first to encourage chain-of-thought reasoning before the LLM scores\n",
    "# --- Fluency ---\n",
    "class FluencyEval(BaseModel):\n",
    "    reason: str = Field(description=\"Explanation for the fluency score\")\n",
    "    fluency: int = Field(description=\"Score from 1-5 for grammar and readability\")\n",
    "\n",
    "FLUENCY_PROMPT = \"\"\"You will be given one summary written for an article. Your task is to rate the summary on one metric.\n",
    "<evaluation_metric>\n",
    "Fluency\n",
    "</evaluation_metric>\n",
    "<evaluation_criteria>\n",
    "Fluency(1-5): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.\n",
    "1: Poor. The summary has many errors that make it hard to understand or sound unnatural.\n",
    "2: Fair. The summary has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n",
    "3: Good. The summary has few errors and is easy to read and follow.\n",
    "4: Very Good. The summary is fluent with no errors.\n",
    "5: Excellent. The summary is highly fluent and well-written with no errors.\n",
    "</evaluation_criteria>\n",
    "<source_text>\n",
    "{article}\n",
    "</source_text>\n",
    "<summarized_text>\n",
    "{summary}\n",
    "</summarized_text>\"\"\"\n",
    "\n",
    "# --- Coherence ---\n",
    "class CoherenceEval(BaseModel):\n",
    "    reason: str = Field(description=\"Explanation for the coherence score\")\n",
    "    coherence: int = Field(description=\"Score from 1-5 for logical structure\")\n",
    "\n",
    "COHERENCE_PROMPT = \"\"\"You will be given one summary written for an article. Your task is to rate the summary on one metric.\n",
    "<evaluation_metric>\n",
    "Coherence\n",
    "</evaluation_metric>\n",
    "<evaluation_criteria>\n",
    "Coherence(1-5) - the collective quality of all sentences. The summary should be well-structured and well-organized, not just a heap of related information, but building from sentence to a coherent body of information about a topic.\n",
    "1: Poor. The summary is disjointed and hard to follow.\n",
    "2: Fair. The summary has some logical gaps.\n",
    "3: Good. The summary is reasonably well-organized.\n",
    "4: Very Good. The summary flows well with clear structure.\n",
    "5: Excellent. The summary is perfectly structured and coherent.\n",
    "</evaluation_criteria>\n",
    "<source_text>\n",
    "{article}\n",
    "</source_text>\n",
    "<summarized_text>\n",
    "{summary}\n",
    "</summarized_text>\"\"\"\n",
    "\n",
    "# --- Conciseness ---\n",
    "class ConcisenessEval(BaseModel):\n",
    "    reason: str = Field(description=\"Explanation for the conciseness score\")\n",
    "    conciseness: int = Field(description=\"Score from 1-5 for brevity\")\n",
    "\n",
    "CONCISENESS_PROMPT = \"\"\"You will be given one summary written for an article. Your task is to rate the summary on one metric.\n",
    "<evaluation_metric>\n",
    "Conciseness\n",
    "</evaluation_metric>\n",
    "<evaluation_criteria>\n",
    "Conciseness(1-5) - Summaries should be concise and to the point using minimal words.\n",
    "1: Poor. The summary is too long and contains unnecessary information.\n",
    "2: Fair. The summary is somewhat verbose.\n",
    "3: Good. The summary is reasonably concise.\n",
    "4: Very Good. The summary is concise with minimal excess.\n",
    "5: Excellent. The summary is very concise and to the point.\n",
    "</evaluation_criteria>\n",
    "<source_text>\n",
    "{article}\n",
    "</source_text>\n",
    "<summarized_text>\n",
    "{summary}\n",
    "</summarized_text>\"\"\"\n",
    "\n",
    "# --- Consistency ---\n",
    "class ConsistencyEval(BaseModel):\n",
    "    reason: str = Field(description=\"Explanation for the consistency score\")\n",
    "    consistency: int = Field(description=\"Score from 1-5 for factual accuracy\")\n",
    "\n",
    "CONSISTENCY_PROMPT = \"\"\"You will be given one summary written for an article. Your task is to rate the summary on one metric.\n",
    "<evaluation_metric>\n",
    "Consistency\n",
    "</evaluation_metric>\n",
    "<evaluation_criteria>\n",
    "Consistency(1-5) - the factual alignment between the summary and the summarized source. A factually consistent summary contains only statements that are entailed by the source document. Penalize summaries that contain hallucinated facts.\n",
    "1: Poor. The summary contains multiple factual errors or hallucinations.\n",
    "2: Fair. The summary has some unsupported claims.\n",
    "3: Good. The summary is mostly factual with minor issues.\n",
    "4: Very Good. The summary is factually accurate.\n",
    "5: Excellent. The summary is perfectly consistent with the source.\n",
    "</evaluation_criteria>\n",
    "<source_text>\n",
    "{article}\n",
    "</source_text>\n",
    "<summarized_text>\n",
    "{summary}\n",
    "</summarized_text>\"\"\"\n",
    "\n",
    "# --- Relevance ---\n",
    "class RelevanceEval(BaseModel):\n",
    "    reason: str = Field(description=\"Explanation for the relevance score\")\n",
    "    relevance: int = Field(description=\"Score from 1-5 for key information coverage\")\n",
    "\n",
    "RELEVANCE_PROMPT = \"\"\"You will be given one summary written for an article. Your task is to rate the summary on one metric.\n",
    "<evaluation_metric>\n",
    "Relevance\n",
    "</evaluation_metric>\n",
    "<evaluation_criteria>\n",
    "Relevance(1-5) - selection of important content from the source. The summary should include only important information from the source document. Penalize summaries which contain redundancies or miss key information.\n",
    "1: Poor. The summary misses most key information.\n",
    "2: Fair. The summary captures some but not all key points.\n",
    "3: Good. The summary covers the main points adequately.\n",
    "4: Very Good. The summary captures all key information.\n",
    "5: Excellent. The summary perfectly captures all important information.\n",
    "</evaluation_criteria>\n",
    "<source_text>\n",
    "{article}\n",
    "</source_text>\n",
    "<summarized_text>\n",
    "{summary}\n",
    "</summarized_text>\"\"\"\n",
    "\n",
    "# Metrics configuration\n",
    "EVAL_METRICS = {\n",
    "    \"fluency\": {\"prompt\": FLUENCY_PROMPT, \"response_format\": FluencyEval},\n",
    "    \"coherence\": {\"prompt\": COHERENCE_PROMPT, \"response_format\": CoherenceEval},\n",
    "    \"conciseness\": {\"prompt\": CONCISENESS_PROMPT, \"response_format\": ConcisenessEval},\n",
    "    \"consistency\": {\"prompt\": CONSISTENCY_PROMPT, \"response_format\": ConsistencyEval},\n",
    "    \"relevance\": {\"prompt\": RELEVANCE_PROMPT, \"response_format\": RelevanceEval},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Evaluation\n",
    "\n",
    "Score every summary on all five metrics using the judge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_name, metric_info in EVAL_METRICS.items():\n",
    "    df[f\"_{metric_name}_response\"] = df.ai.generate_response(\n",
    "        prompt=metric_info[\"prompt\"],\n",
    "        is_prompt_template=True,\n",
    "        conf=judge_conf,\n",
    "        response_format=metric_info[\"response_format\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse structured JSON responses\n",
    "for metric_name in EVAL_METRICS.keys():\n",
    "    df[metric_name] = df[f\"_{metric_name}_response\"].apply(lambda x: json.loads(x)[metric_name])\n",
    "    df[f\"{metric_name}_reason\"] = df[f\"_{metric_name}_response\"].apply(lambda x: json.loads(x)[\"reason\"])\n",
    "\n",
    "display(df[[\"summary\"] + list(EVAL_METRICS.keys())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = list(EVAL_METRICS.keys())\n",
    "avg_scores = {m.capitalize(): df[m].mean() for m in metrics}\n",
    "labels = list(avg_scores.keys())\n",
    "values = list(avg_scores.values())\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n",
    "values_plot = values + values[:1]\n",
    "angles_plot = angles + angles[:1]\n",
    "ax1 = fig.add_subplot(121, polar=True)\n",
    "ax1.plot(angles_plot, values_plot, \"o-\", linewidth=2, color=\"#9b59b6\")\n",
    "ax1.fill(angles_plot, values_plot, alpha=0.25, color=\"#9b59b6\")\n",
    "ax1.set_xticks(angles)\n",
    "ax1.set_xticklabels(labels, size=10)\n",
    "ax1.set_ylim(0, 5)\n",
    "ax1.set_title(\"Summary Quality Radar\", size=12, pad=20)\n",
    "\n",
    "# Bar chart\n",
    "colors = [\"#3498db\", \"#2ecc71\", \"#f39c12\", \"#e74c3c\", \"#9b59b6\"]\n",
    "bars = axes[1].barh(labels, values, color=colors)\n",
    "axes[1].set_xlim(0, 5)\n",
    "axes[1].set_xlabel(\"Score (1-5)\", size=10)\n",
    "axes[1].axvline(x=4, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "for i, (bar, val) in enumerate(zip(bars, values)):\n",
    "    axes[1].text(val + 0.1, i, f\"{val:.2f}\", va=\"center\", fontweight=\"bold\")\n",
    "\n",
    "axes[1].set_title(\"Score Breakdown\", size=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = sum(values) / len(values)\n",
    "print(\"=\" * 60)\n",
    "print(\"  SUMMARIZATION QUALITY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  Samples evaluated: {len(df)}\")\n",
    "print(f\"\\n  Individual Metrics:\")\n",
    "\n",
    "for label, val in zip(labels, values):\n",
    "    status = \"[PASS]\" if val >= 4 else \"[REVIEW]\" if val >= 3.5 else \"[FAIL]\"\n",
    "    print(f\"    {status} {label}: {val:.2f}/5\")\n",
    "\n",
    "print(f\"\\n  {'='*40}\")\n",
    "print(f\"  OVERALL SCORE: {overall:.2f}/5\")\n",
    "print(f\"  {'='*40}\")\n",
    "\n",
    "if overall >= 4.5:\n",
    "    print(\"\\n  Excellent! Summaries are production-ready.\")\n",
    "elif overall >= 4.0:\n",
    "    print(\"\\n  Good quality. Minor improvements possible.\")\n",
    "elif overall >= 3.5:\n",
    "    print(\"\\n  Acceptable. Review low-scoring samples.\")\n",
    "else:\n",
    "    print(\"\\n  Needs improvement. Investigate issues below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-sample breakdown\n",
    "breakdown = df[[\"summary\"] + metrics].copy()\n",
    "breakdown[\"avg_score\"] = breakdown[metrics].mean(axis=1).round(2)\n",
    "breakdown[\"status\"] = breakdown[\"avg_score\"].apply(\n",
    "    lambda x: \"PASS\" if x >= 4 else \"REVIEW\" if x >= 3.5 else \"FAIL\"\n",
    ")\n",
    "breakdown[\"summary\"] = breakdown[\"summary\"].astype(str).str[:120] + \"...\"\n",
    "\n",
    "display(breakdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (Optional) Summarize Multi-Column Data\n",
    "\n",
    "`ai.summarize` can also summarize an entire DataFrame row - synthesizing information across all columns into a single summary. This is useful for structured data like support tickets, sales records, or patient intake forms.\n",
    "\n",
    "**Note:** This is a starter example. It may perform better or worse than baseline depending on your data. Test on your own data and tweak prompts, labels, schema, and model settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tickets = pd.DataFrame({\n",
    "    \"ticket_id\": [\"TKT-4021\", \"TKT-4022\", \"TKT-4023\"],\n",
    "    \"customer\": [\"Sarah Martinez\", \"James Chen\", \"Emily Rodriguez\"],\n",
    "    \"issue\": [\n",
    "        \"\"\"After our company migrated to the Enterprise plan last week, approximately \\\n",
    "60 of our 200 users are unable to access the analytics dashboard. They see a spinner \\\n",
    "that loads indefinitely after login. The problem only affects users who were on the \\\n",
    "legacy 'Viewer' role - admin and editor users are fine. We've confirmed it's not a \\\n",
    "browser or network issue; affected users experience the same behavior on Chrome, \\\n",
    "Firefox, and Edge, and from both office and home networks. Our Q1 board meeting is \\\n",
    "next Tuesday and the CFO needs these dashboards.\"\"\",\n",
    "        \"\"\"Our automated billing pipeline has been double-charging a subset of \\\n",
    "customers since the January 15th platform update. We've identified 340 affected \\\n",
    "accounts totaling $47,200 in duplicate charges. The issue appears to be a race \\\n",
    "condition in the webhook handler - when a payment confirmation arrives within 200ms \\\n",
    "of the initial charge request, the system processes it as a new transaction instead \\\n",
    "of an acknowledgment. We need the duplicate charges reversed and a fix deployed \\\n",
    "before the next billing cycle on February 1st.\"\"\",\n",
    "        \"\"\"Three of our production ML models hosted on your platform started returning \\\n",
    "significantly degraded predictions after the v3.8.2 runtime update on January 20th. \\\n",
    "Our fraud detection model's precision dropped from 94.2% to 67.8%, causing a spike \\\n",
    "in false positives that blocked 1,200 legitimate transactions over the weekend. \\\n",
    "We've traced the issue to a change in how the runtime handles NumPy float32 \\\n",
    "precision - our model weights are being silently upcast to float64, which changes \\\n",
    "the inference behavior. Rolling back to v3.8.1 resolves the issue but we lose \\\n",
    "access to the new batch inference API we've already integrated.\"\"\"\n",
    "    ],\n",
    "    \"priority\": [\"High\", \"Critical\", \"Critical\"],\n",
    "    \"resolution\": [\n",
    "        \"\"\"Root cause identified: the Enterprise migration script did not map legacy \\\n",
    "'Viewer' permissions to the new RBAC system. Ran a backfill script to assign the \\\n",
    "'Dashboard Reader' role to all affected users. Verified access restored for all 60 \\\n",
    "users. Added a pre-migration validation check to prevent recurrence.\"\"\",\n",
    "        \"\"\"Deployed hotfix v2.14.3 that adds idempotency keys to the webhook handler, \\\n",
    "preventing duplicate processing. Initiated batch refund for all 340 affected accounts. \\\n",
    "Refunds will appear within 3-5 business days. Sent personalized apology emails to \\\n",
    "each affected customer with a 10% credit on their next invoice.\"\"\",\n",
    "        \"\"\"Engineering confirmed the float32-to-float64 upcast bug in v3.8.2 runtime. \\\n",
    "Released v3.8.3 patch that preserves original dtype during model loading. Customer \\\n",
    "verified fraud model precision returned to 94.1% after patch. Filed internal incident \\\n",
    "report - adding dtype preservation tests to the CI pipeline to prevent regression.\"\"\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Summarize entire rows - all columns are synthesized automatically\n",
    "df_tickets[\"summary\"] = df_tickets.ai.summarize(conf=executor_conf)\n",
    "\n",
    "# Multi-column summaries - all columns synthesized per row\n",
    "\n",
    "display(df_tickets[[\"ticket_id\", \"summary\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. (Optional) Refine with the `instructions` Parameter\n",
    "\n",
    "The default summaries above should already be high quality. If you want to **steer** the output - for example, to enforce a specific length, tone, or focus - use the `instructions` parameter.\n",
    "\n",
    "Below we show how `instructions` can improve conciseness and compare scores against the defaults.\n",
    "\n",
    "**Note:** This is a starter example. It may perform better or worse than baseline depending on your data. Test on your own data and tweak prompts, labels, schema, and model settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate concise summaries with instructions\n",
    "df[\"summary_concise\"] = df[\"article\"].ai.summarize(\n",
    "    instructions=\"Provide an extremely concise summary in 1-2 sentences. Focus only on the single most important takeaway.\",\n",
    "    conf=executor_conf\n",
    ")\n",
    "\n",
    "# Compare default vs instruction-based summaries\n",
    "comparison = pd.DataFrame({\n",
    "    \"default_summary\": df[\"summary\"],\n",
    "    \"default_summary_words\": df[\"summary\"].astype(str).str.split().str.len(),\n",
    "    \"concise_summary\": df[\"summary_concise\"],\n",
    "    \"concise_summary_words\": df[\"summary_concise\"].astype(str).str.split().str.len()\n",
    "})\n",
    "\n",
    "display(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate conciseness for both variants\n",
    "df[\"_conciseness_default_response\"] = df.ai.generate_response(\n",
    "    prompt=CONCISENESS_PROMPT,\n",
    "    is_prompt_template=True,\n",
    "    conf=judge_conf,\n",
    "    response_format=ConcisenessEval\n",
    ")\n",
    "conciseness_prompt_concise = CONCISENESS_PROMPT.replace(\"{summary}\", \"{summary_concise}\")\n",
    "df[\"_conciseness_instruction_response\"] = df.ai.generate_response(\n",
    "    prompt=conciseness_prompt_concise,\n",
    "    is_prompt_template=True,\n",
    "    conf=judge_conf,\n",
    "    response_format=ConcisenessEval\n",
    ")\n",
    "df[\"conciseness_default\"] = df[\"_conciseness_default_response\"].apply(lambda x: json.loads(x)[\"conciseness\"])\n",
    "df[\"conciseness_instruction\"] = df[\"_conciseness_instruction_response\"].apply(lambda x: json.loads(x)[\"conciseness\"])\n",
    "\n",
    "display(df[[\"summary\", \"summary_concise\", \"conciseness_default\", \"conciseness_instruction\"]])\n",
    "avg_default = df[\"conciseness_default\"].mean()\n",
    "avg_instruction = df[\"conciseness_instruction\"].mean()\n",
    "conciseness_summary = pd.DataFrame({\n",
    "    \"Variant\": [\"Default\", \"With Instructions\"],\n",
    "    \"Avg Conciseness (1-5)\": [round(avg_default, 2), round(avg_instruction, 2)]\n",
    "})\n",
    "conciseness_summary[\"Improvement\"] = conciseness_summary[\"Avg Conciseness (1-5)\"].diff().round(2)\n",
    "\n",
    "display(conciseness_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "x = np.arange(len(df))\n",
    "width = 0.35\n",
    "bars1 = axes[0].bar(x - width/2, df[\"conciseness_default\"], width, label=\"Default\", color=\"#3498db\", alpha=0.8)\n",
    "bars2 = axes[0].bar(x + width/2, df[\"conciseness_instruction\"], width, label=\"With Instructions\", color=\"#2ecc71\", alpha=0.8)\n",
    "axes[0].set_xlabel(\"Sample\")\n",
    "axes[0].set_ylabel(\"Conciseness Score (1-5)\")\n",
    "axes[0].set_title(\"Impact of Instructions on Conciseness\")\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels([f\"Sample {i+1}\" for i in range(len(df))])\n",
    "axes[0].axhline(y=4, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 5.5)\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                    f'{height:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "avg_default = df[\"conciseness_default\"].mean()\n",
    "avg_instruction = df[\"conciseness_instruction\"].mean()\n",
    "improvement = avg_instruction - avg_default\n",
    "categories = [\"Default\", \"With Instructions\"]\n",
    "averages = [avg_default, avg_instruction]\n",
    "bar_colors = [\"#3498db\", \"#2ecc71\"]\n",
    "bars = axes[1].bar(categories, averages, color=bar_colors, alpha=0.8, width=0.5)\n",
    "axes[1].set_ylabel(\"Average Score (1-5)\")\n",
    "axes[1].set_title(\"Average Conciseness Scores\")\n",
    "axes[1].set_ylim(0, 5.5)\n",
    "axes[1].axhline(y=4, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "for i, (bar, val) in enumerate(zip(bars, averages)):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, val + 0.1,\n",
    "                f'{val:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interpreting Results\n",
    "\n",
    "**Important:** These scores are LLM-judge proxies, not final ground truth.\n",
    "For fair comparisons, keep `judge_conf` fixed, change one `executor_conf` setting at a time, and confirm production decisions with human-reviewed samples.\n",
    "\n",
    "### Score Guide\n",
    "\n",
    "| Score | Meaning |\n",
    "|-------|---------| \n",
    "| **4.5-5.0** | Excellent - production ready |\n",
    "| **4.0-4.4** | Good - minor improvements possible |\n",
    "| **3.5-3.9** | Acceptable - review flagged samples |\n",
    "| **< 3.5** | Needs work - see options below |\n",
    "\n",
    "### Troubleshooting Low Scores\n",
    "\n",
    "| Metric | Likely Cause | Fix |\n",
    "|--------|--------------|-----|\n",
    "| Fluency | Source has errors | Clean input data |\n",
    "| Coherence | Disjointed output | Add instructions for structure |\n",
    "| Conciseness | Too verbose | Use `instructions` parameter (see Section 7) |\n",
    "| Consistency | Hallucinations | Critical - review carefully |\n",
    "| Relevance | Missing key info | Check source clarity, add instructions |\n",
    "\n",
    "---\n",
    "\n",
    "### Options for Improving Quality\n",
    "\n",
    "#### Option 1: Use the `instructions` parameter\n",
    "\n",
    "```python\n",
    "# Control length\n",
    "df[\"summary\"] = df[\"article\"].ai.summarize(\n",
    "    instructions=\"Summarize in exactly 2 sentences.\"\n",
    ")\n",
    "\n",
    "# Control style\n",
    "df[\"summary\"] = df[\"article\"].ai.summarize(\n",
    "    instructions=\"Write a casual, friendly summary for social media. Use simple language.\"\n",
    ")\n",
    "\n",
    "# Control focus\n",
    "df[\"summary\"] = df[\"article\"].ai.summarize(\n",
    "    instructions=\"Focus on financial metrics and business outcomes. Ignore technical details.\"\n",
    ")\n",
    "\n",
    "# Combine multiple constraints\n",
    "df[\"summary\"] = df[\"article\"].ai.summarize(\n",
    "    instructions=\"\"\"Create a summary that:\n",
    "    - Is maximum 50 words\n",
    "    - Includes all numerical data\n",
    "    - Is written for C-level executives\n",
    "    - Highlights risks and opportunities\"\"\"\n",
    ")\n",
    "```\n",
    "\n",
    "#### Option 2: Use a larger frontier reasoning model\n",
    "\n",
    "```python\n",
    "custom_conf = aifunc.Conf(model_deployment_name=\"gpt-4.1\")\n",
    "df[\"summary\"] = df[\"article\"].ai.summarize(conf=custom_conf)\n",
    "\n",
    "# Or use gpt-5 reasoning for more cognitive horsepower on harder cases (higher quality, higher cost/latency)\n",
    "advanced_conf = aifunc.Conf(\n",
    "    model_deployment_name=\"gpt-5\",\n",
    "    reasoning_effort=\"medium\",\n",
    "    verbosity=\"low\",\n",
    "    temperature=None,  # reasoning models only support None, openai.NOT_GIVEN or default value of temperature\n",
    ")\n",
    "df[\"summary\"] = df[\"article\"].ai.summarize(conf=advanced_conf)\n",
    "```\n",
    "\n",
    "#### Option 3: Full control with `ai.generate_response`\n",
    "\n",
    "For maximum control, use `ai.generate_response` with a custom `response_format`:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class SummaryResult(BaseModel):\n",
    "    summary: str = Field(description=\"A concise 2-3 sentence summary\")\n",
    "    key_points: List[str] = Field(description=\"The 3 most important takeaways\")\n",
    "    word_count: int = Field(description=\"Number of words in the summary\")\n",
    "\n",
    "df[\"result\"] = df.ai.generate_response(\n",
    "    prompt=\"\"\"Summarize this article in 2-3 sentences: {article}\n",
    "    \n",
    "    Requirements:\n",
    "    - Focus on the main news/findings\n",
    "    - Include key numbers and names\n",
    "    - Keep it under 50 words\n",
    "    - List the 3 most important points\"\"\",\n",
    "    is_prompt_template=True,\n",
    "    response_format=SummaryResult\n",
    ")\n",
    "```\n",
    "\n",
    "Use `Literal` for constrained choices and `Field(description=...)` to guide the model.\n",
    "\n",
    "---\n",
    "\n",
    "## Learn More\n",
    "\n",
    "- [ai.summarize docs](https://learn.microsoft.com/en-us/fabric/data-science/ai-functions/pandas/summarize)\n",
    "- [ai.generate_response docs](https://learn.microsoft.com/en-us/fabric/data-science/ai-functions/pandas/generate-response)\n",
    "- [Configuration options](https://learn.microsoft.com/en-us/fabric/data-science/ai-functions/pandas/configuration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
