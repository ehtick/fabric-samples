{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate ai.fix_grammar(...) Quality\n",
    "\n",
    "This notebook shows how to evaluate the output quality of AI Function `ai.fix_grammar` using **LLM-as-a-Judge** - a technique where a large language model evaluates quality without manually labeled ground truth. This starter notebook uses sample data; replace it with your own data and adapt the eval prompts and criteria as needed.\n",
    "\n",
    "### What You'll Do\n",
    "1. Run grammar correction on sample text with errors\n",
    "2. Use a judge model to score each correction on coherence, consistency, and grammar\n",
    "3. Visualize results and identify samples that need review\n",
    "\n",
    "### Before You Start\n",
    "- **Other AI functions?** Find evaluation notebooks for all AI functions at [aka.ms/fabric-aifunctions-eval-notebooks](https://aka.ms/fabric-aifunctions-eval-notebooks)\n",
    "- **Runtime** - This notebook was made for **Fabric 1.3 runtime**.\n",
    "- **Customize this notebook** - The prompts and evaluation criteria below are a starting point. Adjust them to match your specific use case and quality standards.\n",
    "\n",
    "| Metric | Measures |\n",
    "|--------|----------|\n",
    "| **Coherence** | Structure preserved |\n",
    "| **Consistency** | No content changes |\n",
    "| **Grammar** | Errors fixed |\n",
    "\n",
    "[ai.fix_grammar pandas Documentation](https://learn.microsoft.com/fabric/data-science/ai-functions/pandas/fix-grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "In Fabric 1.3 runtime, pandas AI functions require the openai-python package.\n",
    "\n",
    "See [install instructions for AI Functions](https://learn.microsoft.com/fabric/data-science/ai-functions/overview?tabs=pandas-pyspark%2Cpandas#install-dependencies) for up-to-date information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q openai 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synapse.ml.aifunc as aifunc\n",
    "from pydantic import BaseModel, Field\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# Executor: runs AI functions on your data\n",
    "executor_conf = aifunc.Conf(\n",
    "    model_deployment_name=\"gpt-4.1-mini\",  # see https://aka.ms/fabric-ai-models for other models\n",
    "    reasoning_effort=None,\n",
    "    verbosity=None,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# Judge: evaluates outputs (use a large frontier model with reasoning for best pseudo ground truth)\n",
    "judge_conf = aifunc.Conf(\n",
    "    model_deployment_name=\"gpt-5\",  # see https://aka.ms/fabric-ai-models for other models\n",
    "    reasoning_effort=\"low\",\n",
    "    verbosity=\"low\",\n",
    "    temperature=None,  # reasoning models only support None, openai.NOT_GIVEN or default value of temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Your Data\n",
    "\n",
    "Replace the sample data below with your own text containing grammar errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"\"\"Dear Support Team, I am writting to inform you that my order #AC-34829 have not \\\n",
    "arrived yet even though it was suppose to be delivered last wendsday. I have been \\\n",
    "waiting for over a week and nobody dont seem to know where the package is at. \\\n",
    "Please advise on how to procede with this matter urgently.\"\"\",\n",
    "\n",
    "        \"\"\"Just got the new wireless headphones and honestly there not as good as I \\\n",
    "was expecting them to be. The sound quality is ok but the bluetooth keeps \\\n",
    "disconneting every few minutes which is super anoying. Definately would not \\\n",
    "reccommend these to noone who wants reliable audio.\"\"\",\n",
    "\n",
    "        \"\"\"Meeting notes from Tueday's planning session: the team have decided to \\\n",
    "postpone the product launch untill Q3 becuase the QA results was not satisfactory \\\n",
    "and we need to adress several critical bugs before we can move foreward, also \\\n",
    "marketing needs more time to finalize there campagin materials and coordinate with \\\n",
    "the regional teams.\"\"\",\n",
    "\n",
    "        \"\"\"In my oppinion, the most important factor for economic development in \\\n",
    "developing countrys are education. When peoples have access to good schools, they \\\n",
    "can get better jobs and contributes more to the society. Goverments should invests \\\n",
    "more money in education instead of spending it on other less important things.\"\"\",\n",
    "\n",
    "        \"\"\"We are please to present our proposal for the office renovation project; \\\n",
    "which we believe will significantly improves employee productivity and moral. The \\\n",
    "estimated cost of the project are $450000 over a 18-month timeline, this includes \\\n",
    "new furniture ergonomic workstations and a redesigned break room area.\"\"\"\n",
    "    ]\n",
    "})\n",
    "print(f\"Loaded {len(df)} samples\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fix Grammar\n",
    "\n",
    "`ai.fix_grammar` works out of the box - just call it on a column. Here we use `executor_conf` to keep execution settings explicit and consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix grammar using defaults for function behavior (with executor_conf)\n",
    "df[\"corrected\"] = df[\"text\"].ai.fix_grammar(conf=executor_conf)\n",
    "\n",
    "display(df[[\"text\", \"corrected\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Quality\n",
    "\n",
    "Each correction is scored on 3 metrics (1-5 scale) using G-Eval methodology.\n",
    "\n",
    "> **TIP: XML-formatted prompts** - The evaluation prompts use XML tags like `<evaluation_criteria>` and `<original_text>` to help LLMs distinguish between instructions and data. This improves accuracy. Try this pattern in your own prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"reason\" is first to encourage chain-of-thought reasoning before the LLM scores\n",
    "# --- Coherence ---\n",
    "class CoherenceEval(BaseModel):\n",
    "    reason: str = Field(description=\"Explanation for the coherence score\")\n",
    "    coherence: int = Field(description=\"Score from 1-5 for structure preservation\")\n",
    "\n",
    "COHERENCE_PROMPT = \"\"\"You will evaluate the coherence of a grammar correction.\n",
    "<evaluation_metric>\n",
    "Coherence\n",
    "</evaluation_metric>\n",
    "<evaluation_criteria>\n",
    "Coherence(1-5) - Is the structure preserved with minimal unnecessary changes?\n",
    "A coherent correction maintains the original sentence structure and only changes what is necessary.\n",
    "Penalize corrections that unnecessarily restructure or rephrase the text.\n",
    "1: Poor. The correction completely restructures the sentence unnecessarily.\n",
    "2: Fair. The correction makes several unnecessary structural changes.\n",
    "3: Good. The correction mostly preserves structure with some unnecessary changes.\n",
    "4: Very Good. The correction preserves structure well.\n",
    "5: Excellent. The correction only changes what is necessary.\n",
    "</evaluation_criteria>\n",
    "<original_text>\n",
    "{text}\n",
    "</original_text>\n",
    "<corrected_text>\n",
    "{corrected}\n",
    "</corrected_text>\"\"\"\n",
    "\n",
    "# --- Consistency ---\n",
    "class ConsistencyEval(BaseModel):\n",
    "    reason: str = Field(description=\"Explanation for the consistency score\")\n",
    "    consistency: int = Field(description=\"Score from 1-5 for meaning preservation\")\n",
    "\n",
    "CONSISTENCY_PROMPT = \"\"\"You will evaluate the consistency of a grammar correction.\n",
    "<evaluation_metric>\n",
    "Consistency\n",
    "</evaluation_metric>\n",
    "<evaluation_criteria>\n",
    "Consistency(1-5) - Is the content unchanged? No hallucinated or added text?\n",
    "A consistent correction preserves the original meaning without adding or removing content.\n",
    "Penalize corrections that change the meaning or add new information.\n",
    "1: Poor. The correction significantly changes the meaning or adds content.\n",
    "2: Fair. The correction alters some meaning or adds minor content.\n",
    "3: Good. The correction mostly preserves meaning with minor issues.\n",
    "4: Very Good. The correction preserves meaning accurately.\n",
    "5: Excellent. The correction is perfectly consistent with original meaning.\n",
    "</evaluation_criteria>\n",
    "<original_text>\n",
    "{text}\n",
    "</original_text>\n",
    "<corrected_text>\n",
    "{corrected}\n",
    "</corrected_text>\"\"\"\n",
    "\n",
    "# --- Grammar ---\n",
    "class GrammarEval(BaseModel):\n",
    "    reason: str = Field(description=\"Explanation for the grammar score\")\n",
    "    grammar: int = Field(description=\"Score from 1-5 for error correction quality\")\n",
    "\n",
    "GRAMMAR_PROMPT = \"\"\"You will evaluate the grammar quality of a correction.\n",
    "<evaluation_metric>\n",
    "Grammar\n",
    "</evaluation_metric>\n",
    "<evaluation_criteria>\n",
    "Grammar(1-5) - Are grammar, spelling, and punctuation errors fixed?\n",
    "A good grammar correction should fix all errors in the original text.\n",
    "Penalize corrections that miss errors or introduce new errors.\n",
    "1: Poor. Most errors remain unfixed or new errors introduced.\n",
    "2: Fair. Several errors remain unfixed.\n",
    "3: Good. Most errors fixed with some remaining.\n",
    "4: Very Good. Nearly all errors fixed.\n",
    "5: Excellent. All errors fixed, text is grammatically perfect.\n",
    "</evaluation_criteria>\n",
    "<original_text>\n",
    "{text}\n",
    "</original_text>\n",
    "<corrected_text>\n",
    "{corrected}\n",
    "</corrected_text>\"\"\"\n",
    "\n",
    "EVAL_METRICS = {\n",
    "    \"coherence\": {\"prompt\": COHERENCE_PROMPT, \"response_format\": CoherenceEval},\n",
    "    \"consistency\": {\"prompt\": CONSISTENCY_PROMPT, \"response_format\": ConsistencyEval},\n",
    "    \"grammar\": {\"prompt\": GRAMMAR_PROMPT, \"response_format\": GrammarEval},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM-as-Judge Evaluation ---\n",
    "for metric_name, metric_info in EVAL_METRICS.items():\n",
    "    df[f\"_{metric_name}_response\"] = df.ai.generate_response(\n",
    "        prompt=metric_info[\"prompt\"],\n",
    "        is_prompt_template=True,\n",
    "        conf=judge_conf,\n",
    "        response_format=metric_info[\"response_format\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse structured JSON responses\n",
    "for metric_name in EVAL_METRICS.keys():\n",
    "    df[metric_name] = df[f\"_{metric_name}_response\"].apply(lambda x: json.loads(x)[metric_name])\n",
    "    df[f\"{metric_name}_reason\"] = df[f\"_{metric_name}_response\"].apply(lambda x: json.loads(x)[\"reason\"])\n",
    "\n",
    "display(df[[\"text\", \"corrected\"] + list(EVAL_METRICS.keys())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = list(EVAL_METRICS.keys())\n",
    "avg_scores = {m.capitalize(): df[m].mean() for m in metrics}\n",
    "labels = list(avg_scores.keys())\n",
    "values = list(avg_scores.values())\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart\n",
    "colors = [\"#e74c3c\", \"#f39c12\", \"#2ecc71\"]\n",
    "bars = axes[0].bar(labels, values, color=colors)\n",
    "axes[0].set_ylim(0, 5)\n",
    "axes[0].set_ylabel(\"Score (1-5)\")\n",
    "axes[0].axhline(y=4, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "for bar, val in zip(bars, values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, val + 0.1, f\"{val:.2f}\", ha=\"center\", fontweight=\"bold\")\n",
    "\n",
    "axes[0].set_title(\"Average Scores\")\n",
    "\n",
    "# Radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n",
    "values_plot = values + values[:1]\n",
    "angles_plot = angles + angles[:1]\n",
    "ax2 = fig.add_subplot(122, polar=True)\n",
    "ax2.plot(angles_plot, values_plot, \"o-\", linewidth=2, color=\"#9b59b6\")\n",
    "ax2.fill(angles_plot, values_plot, alpha=0.25, color=\"#9b59b6\")\n",
    "ax2.set_xticks(angles)\n",
    "ax2.set_xticklabels(labels)\n",
    "ax2.set_ylim(0, 5)\n",
    "ax2.set_title(\"Quality Radar\", pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = sum(values) / len(values)\n",
    "print(\"=\" * 60)\n",
    "print(\"  GRAMMAR CORRECTION QUALITY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  Samples evaluated: {len(df)}\")\n",
    "print(f\"\\n  Individual Metrics:\")\n",
    "\n",
    "for label, val in zip(labels, values):\n",
    "    status = \"[PASS]\" if val >= 4 else \"[REVIEW]\" if val >= 3.5 else \"[FAIL]\"\n",
    "    print(f\"    {status} {label}: {val:.2f}/5\")\n",
    "\n",
    "print(f\"\\n  {'='*40}\")\n",
    "print(f\"  OVERALL SCORE: {overall:.2f}/5\")\n",
    "print(f\"  {'='*40}\")\n",
    "\n",
    "if overall >= 4.5:\n",
    "    print(\"\\n  Excellent! Corrections are production-ready.\")\n",
    "elif overall >= 4.0:\n",
    "    print(\"\\n  Good quality. Minor improvements possible.\")\n",
    "elif overall >= 3.5:\n",
    "    print(\"\\n  Acceptable. Review low-scoring samples.\")\n",
    "else:\n",
    "    print(\"\\n  Needs improvement. Investigate issues below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-sample breakdown\n",
    "breakdown = df[[\"text\", \"corrected\"] + metrics].copy()\n",
    "breakdown[\"avg_score\"] = breakdown[metrics].mean(axis=1).round(2)\n",
    "breakdown[\"status\"] = breakdown[\"avg_score\"].apply(\n",
    "    lambda x: \"PASS\" if x >= 4 else \"REVIEW\" if x >= 3.5 else \"FAIL\"\n",
    ")\n",
    "breakdown[\"text\"] = breakdown[\"text\"].astype(str).str[:100] + \"...\"\n",
    "breakdown[\"corrected\"] = breakdown[\"corrected\"].astype(str).str[:100] + \"...\"\n",
    "\n",
    "display(breakdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (Optional) Refinement with `ai.generate_response`\n",
    "\n",
    "Keep `ai.fix_grammar` scores as your baseline, then test a structured custom refinement path for potential improvements and explainability.\n",
    "\n",
    "**Note:** This is a starter example. It may perform better or worse than baseline depending on your data. Test on your own data and tweak prompts, labels, schema, and model settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarRefinement(BaseModel):\n",
    "    reason: str = Field(description=\"Short explanation of key grammar fixes applied\")\n",
    "    corrected_text: str = Field(description=\"Grammatically corrected text that preserves meaning\")\n",
    "\n",
    "df[\"_custom_refinement_response\"] = df.ai.generate_response(\n",
    "    prompt=\"\"\"Fix grammar, spelling, and punctuation in the text below.\n",
    "\n",
    "Requirements:\n",
    "- Preserve meaning and tone\n",
    "- Keep changes minimal and factual\n",
    "- Return a concise reason for the corrections\n",
    "<original_text>\n",
    "{text}\n",
    "</original_text>\"\"\",\n",
    "    is_prompt_template=True,\n",
    "    conf=executor_conf,\n",
    "    response_format=GrammarRefinement\n",
    ")\n",
    "df[\"custom_reason\"] = df[\"_custom_refinement_response\"].apply(lambda x: json.loads(x)[\"reason\"])\n",
    "df[\"custom_corrected\"] = df[\"_custom_refinement_response\"].apply(lambda x: json.loads(x)[\"corrected_text\"])\n",
    "\n",
    "display(df[[\"text\", \"corrected\", \"custom_corrected\", \"custom_reason\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_name, metric_info in EVAL_METRICS.items():\n",
    "    custom_prompt = metric_info[\"prompt\"].replace(\"{corrected}\", \"{custom_corrected}\")\n",
    "    df[f\"_custom_{metric_name}_response\"] = df.ai.generate_response(\n",
    "        prompt=custom_prompt,\n",
    "        is_prompt_template=True,\n",
    "        conf=judge_conf,\n",
    "        response_format=metric_info[\"response_format\"]\n",
    "    )\n",
    "\n",
    "for metric_name in EVAL_METRICS.keys():\n",
    "    df[f\"custom_{metric_name}\"] = df[f\"_custom_{metric_name}_response\"].apply(lambda x: json.loads(x)[metric_name])\n",
    "\n",
    "display(df[[\"text\", \"custom_corrected\"] + [f\"custom_{m}\" for m in metrics]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    \"metric\": [m.capitalize() for m in metrics],\n",
    "    \"baseline\": [df[m].mean() for m in metrics],\n",
    "    \"custom\": [df[f\"custom_{m}\"].mean() for m in metrics],\n",
    "})\n",
    "comparison = pd.concat(\n",
    "    [\n",
    "        comparison,\n",
    "        pd.DataFrame([\n",
    "            {\n",
    "                \"metric\": \"Overall Average\",\n",
    "                \"baseline\": comparison[\"baseline\"].mean(),\n",
    "                \"custom\": comparison[\"custom\"].mean(),\n",
    "            }\n",
    "        ]),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "comparison[\"delta\"] = comparison[\"custom\"] - comparison[\"baseline\"]\n",
    "comparison[[\"baseline\", \"custom\", \"delta\"]] = comparison[[\"baseline\", \"custom\", \"delta\"]].round(3)\n",
    "\n",
    "display(comparison)\n",
    "plot_df = comparison[comparison[\"metric\"] != \"Overall Average\"].set_index(\"metric\")[[\"baseline\", \"custom\"]]\n",
    "ax = plot_df.plot(kind=\"bar\", figsize=(8, 4), color=[\"#3498db\", \"#2ecc71\"])\n",
    "ax.set_ylim(0, 5)\n",
    "ax.set_ylabel(\"Score (1-5)\")\n",
    "ax.set_title(\"Baseline vs Custom\")\n",
    "ax.axhline(y=4, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_metrics = [f\"custom_{m}\" for m in metrics]\n",
    "explainability = df[[\"text\", \"corrected\", \"custom_corrected\", \"custom_reason\"] + metrics + custom_metrics].copy()\n",
    "explainability[\"baseline_avg\"] = explainability[metrics].mean(axis=1).round(2)\n",
    "explainability[\"custom_avg\"] = explainability[custom_metrics].mean(axis=1).round(2)\n",
    "explainability[\"delta\"] = (explainability[\"custom_avg\"] - explainability[\"baseline_avg\"]).round(2)\n",
    "\n",
    "for col in [\"text\", \"corrected\", \"custom_corrected\", \"custom_reason\"]:\n",
    "    explainability[col] = explainability[col].astype(str).str[:120] + \"...\"\n",
    "\n",
    "display(explainability[[\"text\", \"corrected\", \"custom_corrected\", \"custom_reason\", \"baseline_avg\", \"custom_avg\", \"delta\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interpreting Results\n",
    "\n",
    "**Important:** These scores are LLM-judge proxies, not final ground truth.\n",
    "For fair comparisons, keep `judge_conf` fixed, change one `executor_conf` setting at a time, and confirm production decisions with human-reviewed samples.\n",
    "\n",
    "### Score Guide\n",
    "\n",
    "| Score | Meaning |\n",
    "|-------|---------| \n",
    "| **4.5-5.0** | Excellent - production ready |\n",
    "| **4.0-4.4** | Good - minor improvements possible |\n",
    "| **3.5-3.9** | Acceptable - review flagged samples |\n",
    "| **< 3.5** | Needs work - see options below |\n",
    "\n",
    "### Troubleshooting Low Scores\n",
    "\n",
    "| Metric | Likely Cause | Fix |\n",
    "|--------|--------------|-----|\n",
    "| Coherence | Over-correction | Text restructured unnecessarily |\n",
    "| Consistency | Content changes | Meaning added/removed |\n",
    "| Grammar | Missed errors | May need specialized handling |\n",
    "\n",
    "---\n",
    "\n",
    "### Options for Improving Quality\n",
    "\n",
    "#### Option 1: Use a larger frontier reasoning model\n",
    "\n",
    "Larger frontier reasoning models have more cognitive horsepower and can improve quality on harder cases, with higher cost and latency.\n",
    "\n",
    "```python\n",
    "custom_conf = aifunc.Conf(model_deployment_name=\"gpt-4.1\")\n",
    "df[\"corrected\"] = df[\"text\"].ai.fix_grammar(conf=custom_conf)\n",
    "\n",
    "# Or use gpt-5 reasoning for harder cases - more cognitive horsepower, higher cost/latency\n",
    "advanced_conf = aifunc.Conf(\n",
    "    model_deployment_name=\"gpt-5\",\n",
    "    reasoning_effort=\"medium\",\n",
    "    verbosity=\"low\",\n",
    "    temperature=None,  # reasoning models only support None, openai.NOT_GIVEN or default value of temperature\n",
    ")\n",
    "df[\"corrected\"] = df[\"text\"].ai.fix_grammar(conf=advanced_conf)\n",
    "```\n",
    "\n",
    "#### Option 2: Full control with `ai.generate_response`\n",
    "\n",
    "The `ai.fix_grammar` function uses prompts tuned for general use. For full control, use `ai.generate_response` with a custom `response_format`:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class GrammarResult(BaseModel):\n",
    "    corrected_text: str = Field(description=\"The text with all grammar errors fixed\")\n",
    "    changes_made: List[str] = Field(description=\"List of corrections applied\")\n",
    "    error_count: int = Field(description=\"Number of errors that were fixed\")\n",
    "\n",
    "df[\"result\"] = df.ai.generate_response(\n",
    "    prompt=\"\"\"Fix the grammar, spelling, and punctuation in this text: {text}\n",
    "    \n",
    "    Rules:\n",
    "    - Preserve the original meaning and tone\n",
    "    - Only fix actual errors, don't rephrase unnecessarily\n",
    "    - Keep technical terms and proper nouns unchanged\n",
    "    - List all changes you made\"\"\",\n",
    "    is_prompt_template=True,\n",
    "    response_format=GrammarResult\n",
    ")\n",
    "```\n",
    "\n",
    "Use `List[str]` for multi-value fields and `Field(description=...)` to guide the model.\n",
    "\n",
    "---\n",
    "\n",
    "## Learn More\n",
    "\n",
    "- [ai.fix_grammar docs](https://learn.microsoft.com/fabric/data-science/ai-functions/pandas/fix-grammar)\n",
    "- [ai.generate_response docs](https://learn.microsoft.com/fabric/data-science/ai-functions/pandas/generate-response)\n",
    "- [Configuration options](https://learn.microsoft.com/fabric/data-science/ai-functions/pandas/configuration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
