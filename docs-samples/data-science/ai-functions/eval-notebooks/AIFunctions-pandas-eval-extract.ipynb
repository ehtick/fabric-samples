{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate ai.extract(...) Quality\n",
    "\n",
    "This notebook shows how to evaluate the output quality of AI Function `ai.extract` using **LLM-as-a-Judge** - a technique where a large language model evaluates quality without manually labeled ground truth. This starter notebook uses sample data; replace it with your own data and adapt the eval prompts and criteria as needed.\n",
    "\n",
    "### What You'll Do\n",
    "1. Extract entities (people, organizations, locations) from sample text using `ai.extract` defaults\n",
    "2. Use a judge model to score each extraction on consistency and relevance\n",
    "3. Visualize results and identify samples that need review\n",
    "\n",
    "### Before You Start\n",
    "- **Other AI functions?** Find evaluation notebooks for all AI functions at [aka.ms/fabric-aifunctions-eval-notebooks](https://aka.ms/fabric-aifunctions-eval-notebooks)\n",
    "- **Runtime** - This notebook was made for **Fabric 1.3 runtime**.\n",
    "- **Customize this notebook** - The prompts, entity labels, and evaluation criteria below are a starting point. Adjust them to match your specific use case and quality standards.\n",
    "\n",
    "| Metric | Measures |\n",
    "|--------|----------|\n",
    "| **Consistency** | No hallucinated entities |\n",
    "| **Relevance** | Key entities captured |\n",
    "\n",
    "[ai.extract pandas Documentation](https://learn.microsoft.com/en-us/fabric/data-science/ai-functions/pandas/extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "In Fabric 1.3 runtime, pandas AI functions require the openai-python package.\n",
    "\n",
    "See [install instructions for AI Functions](https://learn.microsoft.com/en-us/fabric/data-science/ai-functions/overview?tabs=pandas-pyspark%2Cpandas#install-dependencies) for up-to-date information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q openai 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synapse.ml.aifunc as aifunc\n",
    "from pydantic import BaseModel, Field\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# Executor: runs AI functions on your data\n",
    "executor_conf = aifunc.Conf(\n",
    "    model_deployment_name=\"gpt-4.1-mini\",  # see https://aka.ms/fabric-ai-models for other models\n",
    "    reasoning_effort=None,\n",
    "    verbosity=None,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# Judge: evaluates outputs (use a large frontier model with reasoning for best pseudo ground truth)\n",
    "judge_conf = aifunc.Conf(\n",
    "    model_deployment_name=\"gpt-5\",  # see https://aka.ms/fabric-ai-models for other models\n",
    "    reasoning_effort=\"low\",\n",
    "    verbosity=\"low\",\n",
    "    temperature=None,  # reasoning models only support None, openai.NOT_GIVEN or default value of temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Your Data\n",
    "\n",
    "These samples are intentionally high-signal: each row focuses on one concrete event with explicit people, organizations, and locations. This reduces ambiguity and typically yields much higher relevance scores than noisy multi-topic paragraphs.\n",
    "\n",
    "Replace the sample data and labels below with your own domain data once the workflow is clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define entity labels to extract\n",
    "ENTITY_LABELS = [\"person\", \"organization\", \"location\"]\n",
    "\n",
    "# High-signal samples: one primary event per row with explicit named entities.\n",
    "df = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"\"\"In Seattle, Microsoft vice president Sarah Chen announced that Contoso Retail \\\n",
    "signed a cloud support agreement with Microsoft.\"\"\",\n",
    "\n",
    "        \"\"\"At a press briefing in Geneva, WHO director Dr. Amina Yusuf said UNICEF \\\n",
    "will run a measles campaign with the Kenya Ministry of Health.\"\"\",\n",
    "\n",
    "        \"\"\"In Toronto, Justice Elena Park of the Ontario Superior Court approved \\\n",
    "CleanGrid Energy's settlement with Northwind Power after a six-month review.\"\"\",\n",
    "\n",
    "        \"\"\"In Singapore, DBS Bank CEO Piyush Gupta confirmed that DBS Bank signed \\\n",
    "a fraud analytics partnership with OpenAI.\"\"\",\n",
    "\n",
    "        \"\"\"In Berlin, Dr. Lena Vogel from the Max Planck Institute and engineer \\\n",
    "Jonas Weber from Siemens Healthineers presented a new MRI calibration study.\"\"\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(f\"Loaded {len(df)} samples | Entity labels: {ENTITY_LABELS}\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Entities\n",
    "\n",
    "`ai.extract` works out of the box - just pass string labels. Here we use `executor_conf` so execution settings stay explicit and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract using defaults for function behavior (with executor_conf)\n",
    "extracted = df[\"text\"].ai.extract(*ENTITY_LABELS, conf=executor_conf)\n",
    "\n",
    "df = pd.concat([df, extracted], axis=1)\n",
    "display(df[[\"text\"] + ENTITY_LABELS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. (Optional) Advanced Extraction with `ExtractLabel`\n",
    "\n",
    "For even better relevance coverage, use `aifunc.ExtractLabel` with label-specific descriptions and explicit `max_items` so each row captures all relevant people, organizations, and locations.\n",
    "\n",
    "**Note:** This is a starter example. It may perform better or worse than baseline depending on your data. Test on your own data and tweak prompts, labels, schema, and model settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_extracted = df[\"text\"].ai.extract(\n",
    "    aifunc.ExtractLabel(\n",
    "        label=\"person\",\n",
    "        description=\"All distinct people explicitly named in the text, including titles exactly as written.\",\n",
    "        type=\"string\",\n",
    "        max_items=6,\n",
    "    ),\n",
    "    aifunc.ExtractLabel(\n",
    "        label=\"organization\",\n",
    "        description=\"All explicitly named organizations, including full names and acronyms when present.\",\n",
    "        type=\"string\",\n",
    "        max_items=6,\n",
    "    ),\n",
    "    aifunc.ExtractLabel(\n",
    "        label=\"location\",\n",
    "        description=\"All named geographic locations explicitly mentioned, including cities and countries.\",\n",
    "        type=\"string\",\n",
    "        max_items=4,\n",
    "    ),\n",
    "    aifunc.ExtractLabel(\n",
    "        label=\"role\",\n",
    "        description=\"All professional or institutional roles explicitly tied to named people.\",\n",
    "        type=\"string\",\n",
    "        max_items=6,\n",
    "    ),\n",
    "    conf=executor_conf\n",
    ")\n",
    "\n",
    "# Compare with default string-label extraction\n",
    "display(pd.concat([df[[\"text\"]], advanced_extracted], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clean extraction summary for evaluation\n",
    "def _to_clean_list(value):\n",
    "    if isinstance(value, list):\n",
    "        raw_items = value\n",
    "    elif pd.isna(value):\n",
    "        raw_items = []\n",
    "    elif isinstance(value, str):\n",
    "        text = value.strip()\n",
    "        if not text or text.lower() in {\"none\", \"nan\", \"null\", \"[]\"}:\n",
    "            raw_items = []\n",
    "        elif text.startswith(\"[\") and text.endswith(\"]\"):\n",
    "            try:\n",
    "                parsed = json.loads(text)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    import ast\n",
    "                    parsed = ast.literal_eval(text)\n",
    "                except Exception:\n",
    "                    parsed = text\n",
    "            raw_items = parsed if isinstance(parsed, list) else [parsed]\n",
    "        else:\n",
    "            raw_items = [text]\n",
    "    else:\n",
    "        raw_items = [value]\n",
    "\n",
    "    clean_items = []\n",
    "    for item in raw_items:\n",
    "        item_text = str(item).strip()\n",
    "        if item_text and item_text.lower() not in {\"none\", \"nan\", \"null\"}:\n",
    "            clean_items.append(item_text)\n",
    "\n",
    "    return list(dict.fromkeys(clean_items))\n",
    "\n",
    "\n",
    "def _build_extraction_summary(row, fields):\n",
    "    parts = []\n",
    "    for field in fields:\n",
    "        values = _to_clean_list(row.get(field))\n",
    "        if values:\n",
    "            parts.append(f\"{field}: {', '.join(values)}\")\n",
    "    return \" | \".join(parts) if parts else \"No entities extracted\"\n",
    "\n",
    "\n",
    "df[\"_extracted_summary\"] = df.apply(\n",
    "    lambda row: _build_extraction_summary(row, ENTITY_LABELS), axis=1\n",
    ")\n",
    "display(df[[\"text\"] + ENTITY_LABELS + [\"_extracted_summary\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Quality\n",
    "\n",
    "Each extraction is scored on 2 metrics (1-5 scale) using G-Eval methodology.\n",
    "\n",
    "> **TIP: XML-formatted prompts** - The evaluation prompts use XML tags like `<evaluation_criteria>` and `<source_text>` to help LLMs distinguish between instructions and data. This improves accuracy. Try this pattern in your own prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"reason\" is first to encourage chain-of-thought reasoning before the LLM scores\n",
    "# --- Consistency ---\n",
    "class ConsistencyEval(BaseModel):\n",
    "    reason: str = Field(description=\"Explanation for the consistency score\")\n",
    "    consistency: int = Field(description=\"Score from 1-5 for factual consistency\")\n",
    "\n",
    "\n",
    "CONSISTENCY_PROMPT = \"\"\"You will evaluate the consistency of entity extraction results.\n",
    "<evaluation_metric>\n",
    "Consistency\n",
    "</evaluation_metric>\n",
    "<evaluation_criteria>\n",
    "Consistency(1-5) - Are all extracted entities actually present in the source text?\n",
    "A consistent extraction contains only entities that are explicitly mentioned in the source.\n",
    "Penalize extractions that contain hallucinated or fabricated entities.\n",
    "1: Poor. Multiple extracted entities are not in the source text.\n",
    "2: Fair. Some extracted entities are not supported by the source.\n",
    "3: Good. Most entities are correct with minor issues.\n",
    "4: Very Good. All entities are present in the source.\n",
    "5: Excellent. Perfect extraction with no hallucinations.\n",
    "</evaluation_criteria>\n",
    "<source_text>\n",
    "{text}\n",
    "</source_text>\n",
    "<extracted_entities>\n",
    "{_extracted_summary}\n",
    "</extracted_entities>\"\"\"\n",
    "\n",
    "\n",
    "# --- Relevance ---\n",
    "class RelevanceEval(BaseModel):\n",
    "    reason: str = Field(description=\"Explanation for the relevance score\")\n",
    "    relevance: int = Field(description=\"Score from 1-5 for coverage of key entities\")\n",
    "\n",
    "\n",
    "RELEVANCE_PROMPT = \"\"\"You will evaluate the relevance of entity extraction results.\n",
    "<evaluation_metric>\n",
    "Relevance\n",
    "</evaluation_metric>\n",
    "<evaluation_criteria>\n",
    "Relevance(1-5) - Are the important entities from the source text captured?\n",
    "A relevant extraction identifies the key entities that matter for understanding the text.\n",
    "Penalize extractions that miss important entities.\n",
    "1: Poor. Most important entities are missing.\n",
    "2: Fair. Several key entities are missing.\n",
    "3: Good. Main entities captured with some omissions.\n",
    "4: Very Good. All key entities are captured.\n",
    "5: Excellent. Complete and comprehensive extraction.\n",
    "</evaluation_criteria>\n",
    "<source_text>\n",
    "{text}\n",
    "</source_text>\n",
    "<extracted_entities>\n",
    "{_extracted_summary}\n",
    "</extracted_entities>\"\"\"\n",
    "\n",
    "\n",
    "EVAL_METRICS = {\n",
    "    \"consistency\": {\"prompt\": CONSISTENCY_PROMPT, \"response_format\": ConsistencyEval},\n",
    "    \"relevance\": {\"prompt\": RELEVANCE_PROMPT, \"response_format\": RelevanceEval},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM-as-Judge Evaluation ---\n",
    "for metric_name, metric_info in EVAL_METRICS.items():\n",
    "    df[f\"_{metric_name}_response\"] = df.ai.generate_response(\n",
    "        prompt=metric_info[\"prompt\"],\n",
    "        is_prompt_template=True,\n",
    "        conf=judge_conf,\n",
    "        response_format=metric_info[\"response_format\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse structured JSON responses\n",
    "for metric_name in EVAL_METRICS.keys():\n",
    "    df[metric_name] = df[f\"_{metric_name}_response\"].apply(lambda x: json.loads(x)[metric_name])\n",
    "    df[f\"{metric_name}_reason\"] = df[f\"_{metric_name}_response\"].apply(lambda x: json.loads(x)[\"reason\"])\n",
    "\n",
    "display(df[[\"text\"] + ENTITY_LABELS + list(EVAL_METRICS.keys())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = list(EVAL_METRICS.keys())\n",
    "avg_scores = {m.capitalize(): df[m].mean() for m in metrics}\n",
    "labels = list(avg_scores.keys())\n",
    "values = list(avg_scores.values())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart\n",
    "colors = [\"#3498db\", \"#2ecc71\"]\n",
    "bars = axes[0].bar(labels, values, color=colors)\n",
    "axes[0].set_ylim(0, 5)\n",
    "axes[0].set_ylabel(\"Score (1-5)\")\n",
    "axes[0].axhline(y=4, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "for bar, val in zip(bars, values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, val + 0.1, f\"{val:.2f}\", ha=\"center\", fontweight=\"bold\")\n",
    "axes[0].set_title(\"Average Scores\")\n",
    "\n",
    "# Score distribution\n",
    "axes[1].hist([df[\"consistency\"], df[\"relevance\"]], bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "             label=labels, alpha=0.7, color=colors)\n",
    "axes[1].set_xlabel(\"Score\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(\"Score Distribution\")\n",
    "axes[1].legend()\n",
    "axes[1].set_xticks([1, 2, 3, 4, 5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = sum(values) / len(values)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  ENTITY EXTRACTION QUALITY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  Samples evaluated: {len(df)}\")\n",
    "print(f\"  Entity labels: {ENTITY_LABELS}\")\n",
    "\n",
    "print(f\"\\n  Individual Metrics:\")\n",
    "for label, val in zip(labels, values):\n",
    "    status = \"[PASS]\" if val >= 4 else \"[REVIEW]\" if val >= 3.5 else \"[FAIL]\"\n",
    "    print(f\"    {status} {label}: {val:.2f}/5\")\n",
    "\n",
    "print(f\"\\n  {'='*40}\")\n",
    "print(f\"  OVERALL SCORE: {overall:.2f}/5\")\n",
    "print(f\"  {'='*40}\")\n",
    "\n",
    "if overall >= 4.5:\n",
    "    print(\"\\n  Excellent! Extractions are production-ready.\")\n",
    "elif overall >= 4.0:\n",
    "    print(\"\\n  Good quality. Minor improvements possible.\")\n",
    "elif overall >= 3.5:\n",
    "    print(\"\\n  Acceptable. Review low-scoring samples.\")\n",
    "else:\n",
    "    print(\"\\n  Needs improvement. Investigate issues below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-sample breakdown\n",
    "breakdown = df[[\"text\"] + list(EVAL_METRICS.keys())].copy()\n",
    "breakdown[\"avg_score\"] = breakdown[list(EVAL_METRICS.keys())].mean(axis=1).round(2)\n",
    "breakdown[\"status\"] = breakdown[\"avg_score\"].apply(\n",
    "    lambda x: \"PASS\" if x >= 4 else \"REVIEW\" if x >= 3.5 else \"FAIL\"\n",
    ")\n",
    "breakdown[\"text\"] = breakdown[\"text\"].astype(str).str[:100] + \"...\"\n",
    "\n",
    "display(breakdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (Optional) Compare with `ExtractLabel`\n",
    "\n",
    "Reuse the `advanced_extracted` output from Section 3.1 (built with `aifunc.ExtractLabel`). It uses a richer schema (`person`, `organization`, `location`, `role`) but is judged with the same `EVAL_METRICS` and `judge_conf` as baseline for a fair comparison.\n",
    "\n",
    "**Note:** This is a starter example. It may perform better or worse than baseline depending on your data. Test on your own data and tweak prompts, labels, schema, and model settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the ExtractLabel output from Section 3.1 with its richer schema\n",
    "extractlabel_fields = [c for c in (ENTITY_LABELS + [\"role\"]) if c in advanced_extracted.columns]\n",
    "\n",
    "df[\"_extractlabel_summary\"] = advanced_extracted.apply(\n",
    "    lambda row: _build_extraction_summary(row, extractlabel_fields), axis=1\n",
    ")\n",
    "display(\n",
    "    pd.concat(\n",
    "        [df[[\"text\"]], advanced_extracted[extractlabel_fields], df[[\"_extractlabel_summary\"]]],\n",
    "        axis=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Evaluate ExtractLabel output with the same metrics and judge configuration\n",
    "for metric_name, metric_info in EVAL_METRICS.items():\n",
    "    extractlabel_prompt = metric_info[\"prompt\"].replace(\"{_extracted_summary}\", \"{_extractlabel_summary}\")\n",
    "    df[f\"_{metric_name}_extractlabel_response\"] = df.ai.generate_response(\n",
    "        prompt=extractlabel_prompt,\n",
    "        is_prompt_template=True,\n",
    "        conf=judge_conf,\n",
    "        response_format=metric_info[\"response_format\"]\n",
    "    )\n",
    "    df[f\"{metric_name}_extractlabel\"] = df[f\"_{metric_name}_extractlabel_response\"].apply(\n",
    "        lambda x: json.loads(x)[metric_name]\n",
    "    )\n",
    "\n",
    "baseline_metrics = list(EVAL_METRICS.keys())\n",
    "extractlabel_metrics = [f\"{m}_extractlabel\" for m in baseline_metrics]\n",
    "\n",
    "comparison_scores = pd.DataFrame({\n",
    "    \"Metric\": [m.capitalize() for m in baseline_metrics] + [\"Overall\"],\n",
    "    \"Baseline\": [df[m].mean() for m in baseline_metrics] + [df[baseline_metrics].mean(axis=1).mean()],\n",
    "    \"ExtractLabel\": [df[f\"{m}_extractlabel\"].mean() for m in baseline_metrics] + [df[extractlabel_metrics].mean(axis=1).mean()]\n",
    "})\n",
    "comparison_scores[\"Delta (ExtractLabel - baseline)\"] = (\n",
    "    comparison_scores[\"ExtractLabel\"] - comparison_scores[\"Baseline\"]\n",
    ").round(2)\n",
    "comparison_scores[\"Baseline\"] = comparison_scores[\"Baseline\"].round(2)\n",
    "comparison_scores[\"ExtractLabel\"] = comparison_scores[\"ExtractLabel\"].round(2)\n",
    "\n",
    "display(comparison_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-sample baseline vs ExtractLabel comparison\n",
    "sample_comparison = pd.DataFrame({\n",
    "    \"text\": df[\"text\"].astype(str).str[:100] + \"...\",\n",
    "    \"baseline_avg\": df[baseline_metrics].mean(axis=1).round(2),\n",
    "    \"extractlabel_avg\": df[extractlabel_metrics].mean(axis=1).round(2)\n",
    "})\n",
    "sample_comparison[\"delta\"] = (\n",
    "    sample_comparison[\"extractlabel_avg\"] - sample_comparison[\"baseline_avg\"]\n",
    ").round(2)\n",
    "\n",
    "display(sample_comparison)\n",
    "\n",
    "# Visualize average metric scores for baseline vs ExtractLabel\n",
    "plot_df = comparison_scores[comparison_scores[\"Metric\"] != \"Overall\"].copy()\n",
    "x = np.arange(len(plot_df))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "bars1 = ax.bar(x - width / 2, plot_df[\"Baseline\"], width, label=\"Baseline\", color=\"#3498db\", alpha=0.8)\n",
    "bars2 = ax.bar(x + width / 2, plot_df[\"ExtractLabel\"], width, label=\"ExtractLabel\", color=\"#2ecc71\", alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(plot_df[\"Metric\"])\n",
    "ax.set_ylim(0, 5.5)\n",
    "ax.set_ylabel(\"Average Score (1-5)\")\n",
    "ax.set_title(\"Baseline vs ExtractLabel Scores\")\n",
    "ax.axhline(y=4, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "ax.legend()\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, height + 0.08, f\"{height:.2f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interpreting Results\n",
    "\n",
    "**Important:** These scores are LLM-judge proxies, not final ground truth.\n",
    "For fair comparisons, keep `judge_conf` fixed, change one `executor_conf` setting at a time, and confirm production decisions with human-reviewed samples.\n",
    "\n",
    "### Score Guide\n",
    "\n",
    "| Score | Meaning |\n",
    "|-------|---------|\n",
    "| **4.5-5.0** | Excellent - production ready |\n",
    "| **4.0-4.4** | Good - minor improvements possible |\n",
    "| **3.5-3.9** | Acceptable - review flagged samples |\n",
    "| **< 3.5** | Needs work - see options below |\n",
    "\n",
    "### Troubleshooting Low Scores\n",
    "\n",
    "| Metric | Likely Cause | Fix |\n",
    "|--------|--------------|-----|\n",
    "| Consistency | Hallucinated entities | Critical - review carefully |\n",
    "| Relevance | Missing key entities | Use more specific labels or ExtractLabel |\n",
    "\n",
    "---\n",
    "\n",
    "### Options for Improving Quality\n",
    "\n",
    "#### Option 1: Use ExtractLabel for more control\n",
    "\n",
    "```python\n",
    "import synapse.ml.aifunc as aifunc\n",
    "\n",
    "custom_conf = aifunc.Conf(model_deployment_name=\"gpt-4.1\")\n",
    "df[\"people\"] = df[\"text\"].ai.extract(\n",
    "    aifunc.ExtractLabel(\n",
    "        label=\"person\",\n",
    "        description=\"Full names of people mentioned\",\n",
    "        type=\"string\",\n",
    "        max_items=5\n",
    "    ),\n",
    "    conf=custom_conf\n",
    ")\n",
    "```\n",
    "\n",
    "#### Option 2: Use a larger frontier reasoning model\n",
    "\n",
    "```python\n",
    "custom_conf = aifunc.Conf(model_deployment_name=\"gpt-4.1\")\n",
    "extracted = df[\"text\"].ai.extract(\"person\", \"organization\", conf=custom_conf)\n",
    "\n",
    "# Or use gpt-5 reasoning for more cognitive horsepower on harder cases (higher quality, higher cost/latency)\n",
    "advanced_conf = aifunc.Conf(\n",
    "    model_deployment_name=\"gpt-5\",\n",
    "    reasoning_effort=\"medium\",\n",
    "    verbosity=\"low\",\n",
    "    temperature=None,  # reasoning models only support None, openai.NOT_GIVEN or default value of temperature\n",
    ")\n",
    "extracted = df[\"text\"].ai.extract(\"person\", \"organization\", conf=advanced_conf)\n",
    "```\n",
    "\n",
    "#### Option 3: Full control with `ai.generate_response`\n",
    "\n",
    "For maximum control, use `ai.generate_response` with a custom `response_format`:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class ExtractedEntities(BaseModel):\n",
    "    people: List[str] = Field(description=\"Full names of people, including titles\")\n",
    "    organizations: List[str] = Field(description=\"Company and institution names\")\n",
    "    locations: List[str] = Field(description=\"Cities, countries, and addresses\")\n",
    "    relationships: Optional[List[str]] = Field(\n",
    "        default=None,\n",
    "        description=\"How entities relate to each other\"\n",
    "    )\n",
    "\n",
    "df[\"entities\"] = df.ai.generate_response(\n",
    "    prompt=\"\"\"Extract all named entities from this text: {text}\n",
    "\n",
    "    For each person, include their title if mentioned.\n",
    "    For organizations, include the full official name.\n",
    "    For locations, include city and country if available.\"\"\",\n",
    "    is_prompt_template=True,\n",
    "    response_format=ExtractedEntities\n",
    ")\n",
    "```\n",
    "\n",
    "Use `List[str]` for multi-value fields and `Field(description=...)` to guide the model.\n",
    "\n",
    "---\n",
    "\n",
    "## Learn More\n",
    "\n",
    "- [ai.extract docs](https://learn.microsoft.com/en-us/fabric/data-science/ai-functions/pandas/extract)\n",
    "- [ai.generate_response docs](https://learn.microsoft.com/en-us/fabric/data-science/ai-functions/pandas/generate-response)\n",
    "- [Configuration options](https://learn.microsoft.com/en-us/fabric/data-science/ai-functions/pandas/configuration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
