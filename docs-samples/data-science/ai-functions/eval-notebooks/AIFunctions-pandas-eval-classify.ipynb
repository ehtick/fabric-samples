{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate ai.classify(...) Quality\n",
    "\n",
    "This notebook guides you through evaluating the output quality of AI Function `ai.classify` using **LLM-as-a-Judge** - a technique where a large language model acts as an evaluator to assess quality. In this setup, ground truth labels come from a larger judge model, not human labels. Use this starter notebook as a template: replace the sample data and adapt the evaluation prompts and criteria to your use case.\n",
    "\n",
    "### What You'll Do\n",
    "1. Classify sample text into custom categories\n",
    "2. Use a judge model to evaluate each prediction\n",
    "3. Calculate accuracy, precision, recall, and F1 score\n",
    "4. Review per-class performance\n",
    "\n",
    "### Before You Start\n",
    "- **Other AI functions?** Find evaluation notebooks for all AI functions at [aka.ms/fabric-aifunctions-eval-notebooks](https://aka.ms/fabric-aifunctions-eval-notebooks)\n",
    "- **Runtime** - This notebook was made for **Fabric 1.3 runtime**.\n",
    "- **Customize this notebook** - The prompts and evaluation criteria below are a starting point. Adjust them to match your specific use case and quality standards.\n",
    "\n",
    "| Metric | Measures |\n",
    "|--------|----------|\n",
    "| **Accuracy** | Overall correctness |\n",
    "| **Precision** | Correct predictions per class |\n",
    "| **Recall** | Coverage per class |\n",
    "| **F1 Score** | Balance of precision & recall |\n",
    "\n",
    "[ai.classify pandas Documentation](https://learn.microsoft.com/fabric/data-science/ai-functions/pandas/classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "In Fabric 1.3 runtime, pandas AI functions require the openai-python package.\n",
    "\n",
    "See [install instructions for AI Functions](https://learn.microsoft.com/fabric/data-science/ai-functions/overview?tabs=pandas-pyspark%2Cpandas#install-dependencies) for up-to-date information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q openai 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synapse.ml.aifunc as aifunc\n",
    "from pydantic import BaseModel, Field\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# Executor: runs AI functions on your data\n",
    "executor_conf = aifunc.Conf(\n",
    "    model_deployment_name=\"gpt-4.1-mini\",  # see https://aka.ms/fabric-ai-models for other models\n",
    "    reasoning_effort=None,\n",
    "    verbosity=None,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# Judge: evaluates outputs (use a large frontier model with reasoning for best pseudo ground truth)\n",
    "judge_conf = aifunc.Conf(\n",
    "    model_deployment_name=\"gpt-5\",  # see https://aka.ms/fabric-ai-models for other models\n",
    "    reasoning_effort=\"low\",\n",
    "    verbosity=\"low\",\n",
    "    temperature=None,  # reasoning models only support None, openai.NOT_GIVEN or default value of temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Your Data\n",
    "\n",
    "Replace the sample data and categories below with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your categories\n",
    "CATEGORIES = [\"technical_support\", \"billing\", \"feedback\", \"general_inquiry\"]\n",
    "df = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"\"\"I reset my password and entered the correct 2FA code, but the login page keeps reloading instead of signing me in. \\\n",
    "I tested in Chrome and Safari and got the same result.\"\"\",\n",
    "\n",
    "        \"\"\"I was charged twice this week for the same monthly plan. Please refund the duplicate $49.99 \\\n",
    "charge and send me an itemized invoice for the last three months.\"\"\",\n",
    "\n",
    "        \"\"\"Feature request: please add a bulk export option for reports. Downloading 30 client \\\n",
    "reports one by one at quarter end takes too much time.\"\"\",\n",
    "\n",
    "        \"\"\"We are considering your platform for about 500 employees. Can you share enterprise \\\n",
    "feature differences, SSO support, implementation timeline, and whether a dedicated account manager is included?\"\"\",\n",
    "\n",
    "        \"\"\"Our integration started returning HTTP 504 errors on /v2/batch-process after last week's \\\n",
    "update. Payloads above 5 MB time out around 30 seconds in about 40% of requests.\"\"\",\n",
    "\n",
    "        \"\"\"Kudos to the onboarding team - they helped us migrate data and stayed late before \\\n",
    "go-live. The support was clear and proactive throughout the rollout.\"\"\",\n",
    "       \n",
    "        \"\"\"Ticket #TKT-8842 is still unresolved after 72 hours while our production dashboard is \\\n",
    "down. Premium SLA says Sev-1 responses should be within 4 hours - please escalate immediately.\"\"\"\n",
    "    ]\n",
    "})\n",
    "print(f\"Loaded {len(df)} samples\")\n",
    "print(f\"Categories: {CATEGORIES}\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classify Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"category\"] = df[\"text\"].ai.classify(*CATEGORIES, conf=executor_conf)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "counts = df[\"category\"].value_counts()\n",
    "palette = [\"#33aaaa\", \"#22cc77\", \"#9955bb\", \"#ee7722\", \"#ee4433\"]\n",
    "colors = [palette[i % len(palette)] for i in range(len(counts))]\n",
    "counts.plot(kind=\"barh\", ax=ax, color=colors)\n",
    "ax.set_xlabel(\"Count\")\n",
    "ax.set_title(\"Classification Distribution\")\n",
    "\n",
    "for i, v in enumerate(counts):\n",
    "    ax.text(v + 0.1, i, str(v), va=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Predictions\n",
    "\n",
    "A stronger model (`gpt-5`) judges whether each classification is correct.\n",
    "\n",
    "> **TIP: XML-formatted prompts** - The evaluation prompt uses XML tags like `<evaluation_criteria>` and `<text>` to help LLMs distinguish between instructions and data. This improves accuracy. Try this pattern in your own prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"reason\" is first to encourage chain-of-thought reasoning before the LLM scores\n",
    "class ClassifyEval(BaseModel):\n",
    "    reason: str = Field(description=\"Explanation for the evaluation decision\")\n",
    "    correct: bool = Field(description=\"Whether the predicted category is correct\")\n",
    "    expected_category: str = Field(description=\"The correct category for this text\")\n",
    "\n",
    "# Store categories as a column for template access\n",
    "df[\"_categories\"] = str(CATEGORIES)\n",
    "\n",
    "EVAL_PROMPT = \"\"\"You will evaluate whether a text classification is correct.\n",
    "<evaluation_criteria>\n",
    "Determine if the predicted category is the most appropriate choice from the available categories.\n",
    "Consider the primary intent and topic of the text when evaluating.\n",
    "</evaluation_criteria>\n",
    "<available_categories>\n",
    "{_categories}\n",
    "</available_categories>\n",
    "<text>\n",
    "{text}\n",
    "</text>\n",
    "<predicted_category>\n",
    "{category}\n",
    "</predicted_category>\n",
    "Return whether the prediction is correct, and what the expected category should be.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM-as-Judge Evaluation ---\n",
    "df[\"_eval_response\"] = df.ai.generate_response(\n",
    "    prompt=EVAL_PROMPT,\n",
    "    is_prompt_template=True,\n",
    "    conf=judge_conf,\n",
    "    response_format=ClassifyEval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse structured JSON response\n",
    "df[\"correct\"] = df[\"_eval_response\"].apply(lambda x: json.loads(x)[\"correct\"])\n",
    "df[\"expected_category\"] = df[\"_eval_response\"].apply(lambda x: json.loads(x)[\"expected_category\"])\n",
    "df[\"eval_reason\"] = df[\"_eval_response\"].apply(lambda x: json.loads(x)[\"reason\"])\n",
    "\n",
    "display(df[[\"text\", \"category\", \"expected_category\", \"correct\", \"eval_reason\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Calculate metrics\n",
    "y_true = df[\"expected_category\"]\n",
    "y_pred = df[\"category\"]\n",
    "labels = sorted(set(y_true) | set(y_pred))\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "metrics_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
    "metrics_values = [accuracy, precision, recall, f1]\n",
    "metrics_df = pd.DataFrame({\"Metric\": metrics_names, \"Score\": metrics_values})\n",
    "\n",
    "display(metrics_df.round(3))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart for metrics\n",
    "bars = axes[0].bar(metrics_names, metrics_values, color=[\"#33aaaa\", \"#22cc77\", \"#9955bb\", \"#ee7722\"])\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "axes[0].axhline(y=0.8, color=\"#999999\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "for bar, val in zip(bars, metrics_values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, val + 0.02, f\"{val:.2f}\", ha=\"center\", fontweight=\"bold\")\n",
    "\n",
    "axes[0].set_title(\"Classification Metrics (Macro Average)\")\n",
    "\n",
    "# Pie chart for accuracy\n",
    "correct_count = df[\"correct\"].sum()\n",
    "total_count = len(df)\n",
    "colors = [\"#22cc77\", \"#ee4433\"]\n",
    "counts = [correct_count, total_count - correct_count]\n",
    "axes[1].pie(counts, labels=[\"Correct\", \"Incorrect\"], autopct=\"%1.0f%%\", colors=colors)\n",
    "axes[1].set_title(f\"Overall Accuracy: {accuracy*100:.0f}%\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"],\n",
    "        \"Score\": [accuracy, precision, recall, f1],\n",
    "    }\n",
    ")\n",
    "overall_df[\"Status\"] = overall_df[\"Score\"].apply(\n",
    "    lambda x: \"Excellent\" if x >= 0.8 else \"Good\" if x >= 0.7 else \"Needs Work\"\n",
    ")\n",
    "summary_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"samples_evaluated\": total_count,\n",
    "            \"categories\": \", \".join(CATEGORIES),\n",
    "            \"overall_status\": \"Excellent\" if f1 >= 0.8 else \"Good\" if f1 >= 0.7 else \"Needs Work\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "report = classification_report(y_true, y_pred, zero_division=0, output_dict=True)\n",
    "per_class_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"category\": label,\n",
    "            \"precision\": report[label][\"precision\"],\n",
    "            \"recall\": report[label][\"recall\"],\n",
    "            \"f1_score\": report[label][\"f1-score\"],\n",
    "            \"support\": int(report[label][\"support\"]),\n",
    "        }\n",
    "        for label in labels\n",
    "        if label in report\n",
    "    ]\n",
    ")\n",
    "per_class_df[\"status\"] = per_class_df[\"f1_score\"].apply(\n",
    "    lambda x: \"Excellent\" if x >= 0.8 else \"Good\" if x >= 0.7 else \"Needs Work\"\n",
    ")\n",
    "\n",
    "display(summary_df)\n",
    "\n",
    "display(overall_df.round(3))\n",
    "\n",
    "display(per_class_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show incorrect predictions\n",
    "incorrect = df[~df[\"correct\"]][[\"text\", \"category\", \"expected_category\", \"eval_reason\"]].copy()\n",
    "\n",
    "if len(incorrect) > 0:\n",
    "    incorrect[\"text\"] = incorrect[\"text\"].astype(str).str[:100] + \"...\"\n",
    "else:\n",
    "    incorrect = pd.DataFrame(\n",
    "        [{\"text\": \"All predictions are correct.\", \"category\": \"\", \"expected_category\": \"\", \"eval_reason\": \"\"}]\n",
    "    )\n",
    "\n",
    "display(incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (Optional) Refinement: Baseline vs Custom Classifier\n",
    "\n",
    "Use a custom `ai.generate_response` prompt with structured output (`reason` + `category`) to test whether you can improve quality while adding explainability.\n",
    "\n",
    "**Note:** This is a starter example. It may perform better or worse than baseline depending on your data. Test on your own data and tweak prompts, labels, schema, and model settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomClassifyResult(BaseModel):\n",
    "    reason: str = Field(description=\"Terse reason about which category should be chosen\")\n",
    "    category: str = Field(description=\"One category from the provided category list\")\n",
    "\n",
    "CUSTOM_CLASSIFY_PROMPT = \"\"\"Classify the text into exactly one category from this list:\n",
    "{_categories}\n",
    "\n",
    "<text>\n",
    "{text}\n",
    "</text>\n",
    "\n",
    "Think hard and reason about which one is the best fit and why\n",
    "\"\"\"\n",
    "df[\"_custom_response\"] = df.ai.generate_response(\n",
    "    prompt=CUSTOM_CLASSIFY_PROMPT,\n",
    "    is_prompt_template=True,\n",
    "    response_format=CustomClassifyResult,\n",
    "    conf=executor_conf\n",
    ")\n",
    "df[\"custom_category\"] = (\n",
    "    df[\"_custom_response\"].apply(lambda x: json.loads(x)[\"category\"]).astype(str).str.strip().str.lower().str.replace(\" \", \"_\", regex=False)\n",
    ")\n",
    "df[\"custom_reason\"] = df[\"_custom_response\"].apply(lambda x: json.loads(x)[\"reason\"])\n",
    "\n",
    "display(df[[\"text\", \"category\", \"expected_category\", \"custom_category\", \"custom_reason\"]])\n",
    "\n",
    "custom_precision = precision_score(y_true, df[\"custom_category\"], average=\"macro\", zero_division=0)\n",
    "custom_recall = recall_score(y_true, df[\"custom_category\"], average=\"macro\", zero_division=0)\n",
    "custom_f1 = f1_score(y_true, df[\"custom_category\"], average=\"macro\", zero_division=0)\n",
    "custom_accuracy = accuracy_score(y_true, df[\"custom_category\"])\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"],\n",
    "        \"Baseline (ai.classify)\": [accuracy, precision, recall, f1],\n",
    "        \"Custom (generate_response)\": [custom_accuracy, custom_precision, custom_recall, custom_f1],\n",
    "    }\n",
    ")\n",
    "comparison_df[\"Delta (Custom - Baseline)\"] = (\n",
    "    comparison_df[\"Custom (generate_response)\"] - comparison_df[\"Baseline (ai.classify)\"]\n",
    ")\n",
    "\n",
    "display(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = comparison_df.set_index(\"Metric\")[[\"Baseline (ai.classify)\", \"Custom (generate_response)\"]].plot(\n",
    "    kind=\"bar\", figsize=(8, 4), rot=0, color=[\"#33aaaa\", \"#22cc77\"]\n",
    ")\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Baseline vs Custom Metrics\")\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.2f\", padding=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interpreting Results\n",
    "\n",
    "**Important:** These scores are LLM-judge proxies, not final ground truth.\n",
    "For fair comparisons, keep `judge_conf` fixed, change one `executor_conf` setting at a time, and confirm production decisions with human-reviewed samples.\n",
    "\n",
    "### Metrics Guide\n",
    "\n",
    "| Metric | Target | Meaning |\n",
    "|--------|--------|---------|\n",
    "| **Accuracy** | 90%+ | Overall correctness |\n",
    "| **Precision** | 90%+ | When it predicts X, is it right? |\n",
    "| **Recall** | 90%+ | Does it find all X's? |\n",
    "| **F1 Score** | 90%+ | Balance of precision & recall |\n",
    "\n",
    "| Score | Status |\n",
    "|-------|--------|\n",
    "| **80%+** | Excellent |\n",
    "| **70%-80%** | Good |\n",
    "| **<70%** | Needs Work |\n",
    "\n",
    "### Troubleshooting Low Scores\n",
    "\n",
    "| Issue | Likely Cause | Fix |\n",
    "|-------|--------------|-----|\n",
    "| Low precision on a class | Ambiguous category boundaries | Refine category definitions |\n",
    "| Low recall on a class | Underrepresented in data | Add more examples of that class |\n",
    "| Overall low accuracy | Categories too similar | Merge or clarify overlapping categories |\n",
    "\n",
    "---\n",
    "\n",
    "### Options for Improving Quality\n",
    "\n",
    "#### Option 1: Use a larger frontier reasoning model\n",
    "\n",
    "Larger frontier reasoning models have more cognitive horsepower and can improve quality on harder cases, with higher cost and latency.\n",
    "\n",
    "```python\n",
    "executor_conf = aifunc.Conf(model_deployment_name=\"gpt-4.1\")\n",
    "\n",
    "# Or use gpt-5 reasoning for harder cases - more cognitive horsepower, higher cost/latency\n",
    "executor_conf = aifunc.Conf(\n",
    "    model_deployment_name=\"gpt-5\",\n",
    "    reasoning_effort=\"medium\",\n",
    "    verbosity=\"low\",\n",
    "    temperature=None,  # reasoning models only support None, openai.NOT_GIVEN or default value of temperature\n",
    ")\n",
    "df[\"category\"] = df[\"text\"].ai.classify(*CATEGORIES, conf=executor_conf)\n",
    "```\n",
    "\n",
    "#### Option 2: Build a custom classifier with `ai.generate_response`\n",
    "\n",
    "The `ai.classify` function uses prompts tuned for general use. A custom classify function built with `ai.generate_response` can improve performance on domain-specific categories:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class ClassificationResult(BaseModel):\n",
    "    reason: str = Field(description=\"Explanation for why this category was chosen\")\n",
    "    category: Literal[\"technical_support\", \"billing\", \"feedback\", \"general_inquiry\"] = Field(\n",
    "        description=\"The most appropriate category for the customer message\"\n",
    "    )\n",
    "    confidence: float = Field(description=\"Confidence score from 0 to 1\")\n",
    "\n",
    "df[\"result\"] = df.ai.generate_response(\n",
    "    prompt=\"\"\"Classify this customer message into one of these categories:\n",
    "    - technical_support: Issues with product functionality, bugs, how-to questions\n",
    "    - billing: Payment, refunds, subscription, pricing issues\n",
    "    - feedback: Compliments, complaints, or suggestions about the product\n",
    "    - general_inquiry: Questions about products, services, or company info\n",
    "    \n",
    "    Message: {text}\"\"\",\n",
    "    is_prompt_template=True,\n",
    "    conf=executor_conf,\n",
    "    response_format=ClassificationResult\n",
    ")\n",
    "```\n",
    "\n",
    "Use `Literal` for constrained choices and `Field(description=...)` to guide the model.\n",
    "\n",
    "---\n",
    "\n",
    "## Learn More\n",
    "\n",
    "- [ai.classify docs](https://learn.microsoft.com/fabric/data-science/ai-functions/pandas/classify)\n",
    "- [ai.generate_response docs](https://learn.microsoft.com/fabric/data-science/ai-functions/pandas/generate-response)\n",
    "- [Configuration options](https://learn.microsoft.com/fabric/data-science/ai-functions/pandas/configuration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
