{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate ai.translate(...) Quality\n",
    "\n",
    "This notebook shows how to evaluate the output quality of AI Function `ai.translate` using **LLM-as-a-Judge** - a technique where a large language model evaluates quality without manually labeled ground truth. This starter notebook uses sample data; replace it with your own data and adapt the eval prompts and criteria as needed.\n",
    "\n",
    "### What You'll Do\n",
    "1. Translate sample text to a target language using `ai.translate` defaults\n",
    "2. Use a judge model to score each translation on three quality metrics\n",
    "3. Visualize results with radar and bar charts\n",
    "4. Identify samples that need review\n",
    "\n",
    "### Before You Start\n",
    "- **Other AI functions?** Find evaluation notebooks for all AI functions at [aka.ms/fabric-aifunctions-eval-notebooks](https://aka.ms/fabric-aifunctions-eval-notebooks)\n",
    "- **Runtime** - This notebook was made for **Fabric 1.3 runtime**.\n",
    "- **Customize this notebook** - The prompts and evaluation criteria below are a starting point. Adjust them to match your specific use case, target language, and quality standards.\n",
    "\n",
    "| Metric | Measures |\n",
    "|--------|----------|\n",
    "| **Coherence** | Structure preserved |\n",
    "| **Consistency** | No omissions/additions |\n",
    "| **Translation** | Accurate & natural |\n",
    "\n",
    "[ai.translate pandas Documentation](https://learn.microsoft.com/fabric/data-science/ai-functions/pandas/translate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "In Fabric 1.3 runtime, pandas AI functions require the openai-python package.\n",
    "\n",
    "See [install instructions for AI Functions](https://learn.microsoft.com/fabric/data-science/ai-functions/overview?tabs=pandas-pyspark%2Cpandas#install-dependencies) for up-to-date information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q openai 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synapse.ml.aifunc as aifunc\n",
    "from pydantic import BaseModel, Field\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# Executor: runs AI functions on your data\n",
    "executor_conf = aifunc.Conf(\n",
    "    model_deployment_name=\"gpt-4.1-mini\",  # see https://aka.ms/fabric-ai-models for other models\n",
    "    reasoning_effort=None,\n",
    "    verbosity=None,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# Judge: evaluates outputs (use a large frontier model with reasoning for best pseudo ground truth)\n",
    "judge_conf = aifunc.Conf(\n",
    "    model_deployment_name=\"gpt-5\",  # see https://aka.ms/fabric-ai-models for other models\n",
    "    reasoning_effort=\"low\",\n",
    "    verbosity=\"low\",\n",
    "    temperature=None,  # reasoning models only support None, openai.NOT_GIVEN or default value of temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Your Data\n",
    "\n",
    "Replace the sample data and target language below with your own.\n",
    "\n",
    "`ai.translate` automatically detects the source language, so your input can be **mixed-language** - the samples below include Italian, French, German, English, and Portuguese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your target language\n",
    "TARGET_LANGUAGE = \"English\"\n",
    "df = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"\"\"Tutti gli articoli acquistati dal nostro negozio online possono essere \\\n",
    "restituiti entro 30 giorni di calendario dalla data di consegna originale. Per \\\n",
    "avere diritto a un rimborso completo, la merce deve essere nella confezione \\\n",
    "originale con tutte le etichette attaccate e non deve mostrare segni di usura \\\n",
    "o danni. I rimborsi vengono elaborati sul metodo di pagamento originale entro \\\n",
    "5-7 giorni lavorativi dopo la ricezione e l'ispezione dell'articolo restituito.\"\"\",\n",
    "\n",
    "        \"\"\"Nous vous rappelons que votre rendez-vous avec le Dr. Elena Vasquez est \\\n",
    "prévu pour jeudi 14 mars à 14h30 dans la suite 410 du Westfield Medical Plaza. \\\n",
    "Veuillez arriver 15 minutes en avance pour remplir vos documents d'admission. \\\n",
    "Apportez votre carte d'assurance, une pièce d'identité avec photo et la liste \\\n",
    "de tous les médicaments que vous prenez actuellement, y compris le dosage et \\\n",
    "la fréquence.\"\"\",\n",
    "\n",
    "        \"\"\"Durch den Zugang zu oder die Nutzung dieses Dienstes erklärt sich der \\\n",
    "Nutzer mit diesen Allgemeinen Geschäftsbedingungen einverstanden. Das Unternehmen \\\n",
    "behält sich das Recht vor, diese Bedingungen jederzeit ohne vorherige Ankündigung \\\n",
    "zu ändern; die fortgesetzte Nutzung des Dienstes nach solchen Änderungen gilt als \\\n",
    "Zustimmung des Nutzers zu den geänderten Bedingungen. Streitigkeiten aus dieser \\\n",
    "Vereinbarung unterliegen den Gesetzen des Staates Delaware und werden ausschließlich \\\n",
    "vor den Gerichten von Wilmington verhandelt.\"\"\",\n",
    "\n",
    "        \"\"\"Welcome to Acme Analytics! Your team account has been successfully created. \\\n",
    "To get started, invite your colleagues from the Settings > Team Members page - each \\\n",
    "member will receive an activation email valid for 48 hours. We recommend connecting \\\n",
    "your first data source within the onboarding wizard, which supports CSV uploads, \\\n",
    "direct database connections via JDBC, and REST API integrations out of the box.\"\"\",\n",
    "\n",
    "        \"\"\"Nossa API pública aplica um limite de 1.000 requisições por minuto por \\\n",
    "chave de API. Se você exceder esse limite, o servidor responderá com HTTP 429 \\\n",
    "(Too Many Requests) e incluirá um cabeçalho Retry-After indicando o número de \\\n",
    "segundos a aguardar antes de tentar novamente. Para necessidades de maior \\\n",
    "throughput, considere fazer upgrade para nosso plano Enterprise, que oferece \\\n",
    "limites configuráveis de até 50.000 requisições por minuto e isolamento \\\n",
    "dedicado de endpoint.\"\"\"\n",
    "    ]\n",
    "})\n",
    "df[\"_target_lang\"] = TARGET_LANGUAGE\n",
    "print(f\"Loaded {len(df)} samples - translating to {TARGET_LANGUAGE}\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translate Text\n",
    "\n",
    "`ai.translate` works out of the box - just pass a target language. It **auto-detects the source language**, so mixed-language input is handled seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate using defaults for function behavior (with executor_conf)\n",
    "df[\"translation\"] = df[\"text\"].ai.translate(TARGET_LANGUAGE.lower(), conf=executor_conf)\n",
    "\n",
    "display(df[[\"text\", \"translation\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Quality\n",
    "\n",
    "Each translation is scored on 3 metrics (1-5 scale) using G-Eval methodology.\n",
    "\n",
    "> **TIP: XML-formatted prompts** - The evaluation prompts use XML tags like `<evaluation_criteria>` and `<source_text>` to help LLMs distinguish between instructions and data. This improves accuracy. Try this pattern in your own prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"reason\" is first to encourage chain-of-thought reasoning before the LLM scores\n",
    "# --- Coherence ---\n",
    "class CoherenceEval(BaseModel):\n",
    "    reason: str = Field(description=\"Explanation for the coherence score\")\n",
    "    coherence: int = Field(description=\"Score from 1-5 for structure preservation\")\n",
    "\n",
    "COHERENCE_PROMPT = \"\"\"You will evaluate the coherence of a translation.\n",
    "<evaluation_metric>\n",
    "Coherence\n",
    "</evaluation_metric>\n",
    "<evaluation_criteria>\n",
    "Coherence(1-5) - Is the structure similar to the source?\n",
    "A coherent translation maintains the sentence structure and flow of the original.\n",
    "Penalize translations that unnecessarily restructure or rearrange content.\n",
    "1: Poor. The translation completely restructures the sentence.\n",
    "2: Fair. The translation has significant structural differences.\n",
    "3: Good. The translation mostly preserves structure with some changes.\n",
    "4: Very Good. The translation closely follows source structure.\n",
    "5: Excellent. The translation perfectly mirrors source structure where appropriate.\n",
    "</evaluation_criteria>\n",
    "<target_language>\n",
    "{_target_lang}\n",
    "</target_language>\n",
    "<source_text>\n",
    "{text}\n",
    "</source_text>\n",
    "<translation>\n",
    "{translation}\n",
    "</translation>\"\"\"\n",
    "\n",
    "# --- Consistency ---\n",
    "class ConsistencyEval(BaseModel):\n",
    "    reason: str = Field(description=\"Explanation for the consistency score\")\n",
    "    consistency: int = Field(description=\"Score from 1-5 for content preservation\")\n",
    "\n",
    "CONSISTENCY_PROMPT = \"\"\"You will evaluate the consistency of a translation.\n",
    "<evaluation_metric>\n",
    "Consistency\n",
    "</evaluation_metric>\n",
    "<evaluation_criteria>\n",
    "Consistency(1-5) - Is all content translated without additions or omissions?\n",
    "A consistent translation includes all information from the source without adding new content.\n",
    "Penalize translations that omit parts or add information not in the original.\n",
    "1: Poor. Significant content is missing or added.\n",
    "2: Fair. Some content is missing or extra content added.\n",
    "3: Good. Minor omissions or additions.\n",
    "4: Very Good. All content preserved with minimal changes.\n",
    "5: Excellent. Perfect content preservation, nothing added or omitted.\n",
    "</evaluation_criteria>\n",
    "<target_language>\n",
    "{_target_lang}\n",
    "</target_language>\n",
    "<source_text>\n",
    "{text}\n",
    "</source_text>\n",
    "<translation>\n",
    "{translation}\n",
    "</translation>\"\"\"\n",
    "\n",
    "# --- Translation Quality ---\n",
    "class TranslationEval(BaseModel):\n",
    "    reason: str = Field(description=\"Explanation for the translation quality score\")\n",
    "    translation_quality: int = Field(description=\"Score from 1-5 for accuracy and naturalness\")\n",
    "\n",
    "TRANSLATION_PROMPT = \"\"\"You will evaluate the quality of a translation.\n",
    "<evaluation_metric>\n",
    "Translation Quality\n",
    "</evaluation_metric>\n",
    "<evaluation_criteria>\n",
    "Translation Quality(1-5) - Is the translation accurate and natural-sounding?\n",
    "A high-quality translation conveys the correct meaning and reads naturally in the target language.\n",
    "Penalize translations with incorrect meaning or unnatural phrasing.\n",
    "1: Poor. The translation is incorrect or incomprehensible.\n",
    "2: Fair. The translation has significant errors or sounds very unnatural.\n",
    "3: Good. The translation is mostly correct but has some awkward phrasing.\n",
    "4: Very Good. The translation is accurate and reads well.\n",
    "5: Excellent. The translation is perfectly accurate and sounds completely natural.\n",
    "</evaluation_criteria>\n",
    "<target_language>\n",
    "{_target_lang}\n",
    "</target_language>\n",
    "<source_text>\n",
    "{text}\n",
    "</source_text>\n",
    "<translation>\n",
    "{translation}\n",
    "</translation>\"\"\"\n",
    "\n",
    "EVAL_METRICS = {\n",
    "    \"coherence\": {\"prompt\": COHERENCE_PROMPT, \"response_format\": CoherenceEval},\n",
    "    \"consistency\": {\"prompt\": CONSISTENCY_PROMPT, \"response_format\": ConsistencyEval},\n",
    "    \"translation_quality\": {\"prompt\": TRANSLATION_PROMPT, \"response_format\": TranslationEval},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM-as-Judge Evaluation ---\n",
    "for metric_name, metric_info in EVAL_METRICS.items():\n",
    "    df[f\"_{metric_name}_response\"] = df.ai.generate_response(\n",
    "        prompt=metric_info[\"prompt\"],\n",
    "        is_prompt_template=True,\n",
    "        conf=judge_conf,\n",
    "        response_format=metric_info[\"response_format\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse structured JSON responses\n",
    "for metric_name in EVAL_METRICS.keys():\n",
    "    df[metric_name] = df[f\"_{metric_name}_response\"].apply(lambda x: json.loads(x)[metric_name])\n",
    "    df[f\"{metric_name}_reason\"] = df[f\"_{metric_name}_response\"].apply(lambda x: json.loads(x)[\"reason\"])\n",
    "\n",
    "display(df[[\"text\", \"translation\"] + list(EVAL_METRICS.keys())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = list(EVAL_METRICS.keys())\n",
    "avg_scores = {m.replace(\"_\", \" \").title(): df[m].mean() for m in metrics}\n",
    "labels = list(avg_scores.keys())\n",
    "values = list(avg_scores.values())\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[1].remove()\n",
    "axes[1] = fig.add_subplot(1, 2, 2, polar=True)\n",
    "\n",
    "# Bar chart\n",
    "colors = [\"#0077aa\", \"#22cc77\", \"#9955bb\"]\n",
    "bars = axes[0].barh(labels, values, color=colors)\n",
    "axes[0].set_xlim(0, 5)\n",
    "axes[0].set_xlabel(\"Score (1-5)\", size=10)\n",
    "axes[0].axvline(x=4, color=\"#999999\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "for i, (bar, val) in enumerate(zip(bars, values)):\n",
    "    axes[0].text(val + 0.1, i, f\"{val:.2f}\", va=\"center\", fontweight=\"bold\")\n",
    "\n",
    "axes[0].set_title(f\"Score Breakdown ({TARGET_LANGUAGE})\", size=12)\n",
    "\n",
    "# Radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n",
    "values_plot = values + values[:1]\n",
    "angles_plot = angles + angles[:1]\n",
    "axes[1].plot(angles_plot, values_plot, \"o-\", linewidth=2, color=\"#9955bb\")\n",
    "axes[1].fill(angles_plot, values_plot, alpha=0.25, color=\"#9955bb\")\n",
    "axes[1].set_xticks(angles)\n",
    "axes[1].set_xticklabels(labels, size=10)\n",
    "axes[1].set_ylim(0, 5)\n",
    "axes[1].set_title(\"Translation Quality Radar\", size=12, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = sum(values) / len(values)\n",
    "print(\"=\" * 60)\n",
    "print(f\"  TRANSLATION QUALITY REPORT (to {TARGET_LANGUAGE})\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  Samples evaluated: {len(df)}\")\n",
    "print(f\"\\n  Individual Metrics:\")\n",
    "\n",
    "for label, val in zip(labels, values):\n",
    "    status = \"[PASS]\" if val >= 4 else \"[REVIEW]\" if val >= 3.5 else \"[FAIL]\"\n",
    "    print(f\"    {status} {label}: {val:.2f}/5\")\n",
    "\n",
    "print(f\"\\n  {'='*40}\")\n",
    "print(f\"  OVERALL SCORE: {overall:.2f}/5\")\n",
    "print(f\"  {'='*40}\")\n",
    "\n",
    "if overall >= 4.5:\n",
    "    print(\"\\n  Excellent! Translations are production-ready.\")\n",
    "elif overall >= 4.0:\n",
    "    print(\"\\n  Good quality. Minor improvements possible.\")\n",
    "elif overall >= 3.5:\n",
    "    print(\"\\n  Acceptable. Review low-scoring samples.\")\n",
    "else:\n",
    "    print(\"\\n  Needs improvement. Investigate issues below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-sample breakdown\n",
    "breakdown = df[[\"text\", \"translation\"] + metrics].copy()\n",
    "breakdown[\"avg_score\"] = breakdown[metrics].mean(axis=1).round(2)\n",
    "breakdown[\"status\"] = breakdown[\"avg_score\"].apply(\n",
    "    lambda x: \"PASS\" if x >= 4 else \"REVIEW\" if x >= 3.5 else \"FAIL\"\n",
    ")\n",
    "breakdown[\"text\"] = breakdown[\"text\"].astype(str).str[:100] + \"...\"\n",
    "breakdown[\"translation\"] = breakdown[\"translation\"].astype(str).str[:100] + \"...\"\n",
    "\n",
    "display(breakdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (Optional) Structured Explainability with `ai.generate_response`\n",
    "\n",
    "This optional section keeps `ai.translate` scores as the baseline and tests a custom translation path with `ai.generate_response`. We use the same `judge_conf` prompts and metrics so baseline and custom outputs are directly comparable.\n",
    "\n",
    "**Note:** This is a starter example. It may perform better or worse than baseline depending on your data. Test on your own data and tweak prompts, labels, schema, and model settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplainableTranslation(BaseModel):\n",
    "    reason: str = Field(description=\"Brief explanation of key translation choices\")\n",
    "    translation: str = Field(description=\"Translated text in the target language\")\n",
    "\n",
    "df[\"_custom_translation_response\"] = df.ai.generate_response(\n",
    "    prompt=\"\"\"Translate the following text into {_target_lang}.\n",
    "\n",
    "Return a concise reason and the translation.\n",
    "<source_text>\n",
    "{text}\n",
    "</source_text>\"\"\",\n",
    "    is_prompt_template=True,\n",
    "    conf=executor_conf,\n",
    "    response_format=ExplainableTranslation\n",
    ")\n",
    "df[\"custom_translation_reason\"] = df[\"_custom_translation_response\"].apply(lambda x: json.loads(x)[\"reason\"])\n",
    "df[\"custom_translation\"] = df[\"_custom_translation_response\"].apply(lambda x: json.loads(x)[\"translation\"])\n",
    "\n",
    "display(df[[\"text\", \"translation\", \"custom_translation\", \"custom_translation_reason\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = list(EVAL_METRICS.keys())\n",
    "df_custom_eval = df[[\"text\", \"_target_lang\", \"custom_translation\"]].rename(\n",
    "    columns={\"custom_translation\": \"translation\"}\n",
    "\n",
    ").copy()\n",
    "\n",
    "for metric_name, metric_info in EVAL_METRICS.items():\n",
    "    df_custom_eval[f\"_custom_{metric_name}_response\"] = df_custom_eval.ai.generate_response(\n",
    "        prompt=metric_info[\"prompt\"],\n",
    "        is_prompt_template=True,\n",
    "        conf=judge_conf,\n",
    "        response_format=metric_info[\"response_format\"]\n",
    "    )\n",
    "\n",
    "for metric_name in EVAL_METRICS.keys():\n",
    "    df[f\"custom_{metric_name}\"] = df_custom_eval[f\"_custom_{metric_name}_response\"].apply(\n",
    "        lambda x: json.loads(x)[metric_name]\n",
    "    )\n",
    "\n",
    "display(df[[\"text\"] + [f\"custom_{m}\" for m in metrics]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    \"Metric\": [m.replace(\"_\", \" \").title() for m in metrics],\n",
    "    \"Baseline\": [df[m].mean() for m in metrics],\n",
    "    \"Custom\": [df[f\"custom_{m}\"].mean() for m in metrics],\n",
    "})\n",
    "comparison_df[\"Delta\"] = comparison_df[\"Custom\"] - comparison_df[\"Baseline\"]\n",
    "overall_row = pd.DataFrame([{\n",
    "    \"Metric\": \"Overall Average\",\n",
    "    \"Baseline\": comparison_df[\"Baseline\"].mean(),\n",
    "    \"Custom\": comparison_df[\"Custom\"].mean(),\n",
    "    \"Delta\": comparison_df[\"Delta\"].mean(),\n",
    "}])\n",
    "comparison_df = pd.concat([comparison_df, overall_row], ignore_index=True)\n",
    "comparison_df[[\"Baseline\", \"Custom\", \"Delta\"]] = comparison_df[[\"Baseline\", \"Custom\", \"Delta\"]].round(2)\n",
    "\n",
    "display(comparison_df)\n",
    "plot_df = comparison_df[comparison_df[\"Metric\"] != \"Overall Average\"].copy()\n",
    "x = np.arange(len(plot_df))\n",
    "width = 0.35\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(x - width / 2, plot_df[\"Baseline\"], width, label=\"Baseline\", color=\"#0077aa\")\n",
    "ax.bar(x + width / 2, plot_df[\"Custom\"], width, label=\"Custom\", color=\"#22cc77\")\n",
    "ax.axhline(y=4, color=\"#999999\", linestyle=\"--\", alpha=0.5)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(plot_df[\"Metric\"])\n",
    "ax.set_ylim(0, 5)\n",
    "ax.set_ylabel(\"Average score (1-5)\")\n",
    "ax.set_title(\"Baseline vs Custom Metric Averages\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_explainability = df[\n",
    "    [\"text\", \"translation\", \"custom_translation\", \"custom_translation_reason\"]\n",
    "    + metrics\n",
    "    + [f\"custom_{m}\" for m in metrics]\n",
    "\n",
    "].copy()\n",
    "sample_explainability[\"baseline_avg\"] = sample_explainability[metrics].mean(axis=1).round(2)\n",
    "sample_explainability[\"custom_avg\"] = sample_explainability[[f\"custom_{m}\" for m in metrics]].mean(axis=1).round(2)\n",
    "sample_explainability[\"delta\"] = (sample_explainability[\"custom_avg\"] - sample_explainability[\"baseline_avg\"]).round(2)\n",
    "sample_explainability[\"text\"] = sample_explainability[\"text\"].astype(str).str[:90] + \"...\"\n",
    "sample_explainability[\"translation\"] = sample_explainability[\"translation\"].astype(str).str[:90] + \"...\"\n",
    "sample_explainability[\"custom_translation\"] = sample_explainability[\"custom_translation\"].astype(str).str[:90] + \"...\"\n",
    "sample_explainability[\"custom_translation_reason\"] = sample_explainability[\"custom_translation_reason\"].astype(str).str[:120]\n",
    "\n",
    "display(\n",
    "    sample_explainability[\n",
    "        [\n",
    "            \"text\",\n",
    "            \"baseline_avg\",\n",
    "            \"custom_avg\",\n",
    "            \"delta\",\n",
    "            \"custom_translation_reason\",\n",
    "            \"translation\",\n",
    "            \"custom_translation\",\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interpreting Results\n",
    "\n",
    "**Important:** These scores are LLM-judge proxies, not final ground truth.\n",
    "For fair comparisons, keep `judge_conf` fixed, change one `executor_conf` setting at a time, and confirm production decisions with human-reviewed samples.\n",
    "\n",
    "### Score Guide\n",
    "\n",
    "| Score | Meaning |\n",
    "|-------|---------| \n",
    "| **4.5-5.0** | Excellent - production ready |\n",
    "| **4.0-4.4** | Good - minor improvements possible |\n",
    "| **3.5-3.9** | Acceptable - review flagged samples |\n",
    "| **< 3.5** | Needs work - see options below |\n",
    "\n",
    "### Troubleshooting Low Scores\n",
    "\n",
    "| Metric | Likely Cause | Fix |\n",
    "|--------|--------------|-----|\n",
    "| Coherence | Structural changes | May be necessary for target language grammar |\n",
    "| Consistency | Missing/added content | Check for omissions or hallucinated additions |\n",
    "| Translation Quality | Accuracy issues | Check domain terminology, try a larger model |\n",
    "\n",
    "---\n",
    "\n",
    "### Options for Improving Quality\n",
    "\n",
    "#### Option 1: Use a larger frontier reasoning model\n",
    "\n",
    "```python\n",
    "custom_conf = aifunc.Conf(model_deployment_name=\"gpt-4.1\")\n",
    "df[\"translation\"] = df[\"text\"].ai.translate(\"english\", conf=custom_conf)\n",
    "\n",
    "# Or use gpt-5 reasoning for more cognitive horsepower on harder cases (higher quality, higher cost/latency)\n",
    "advanced_conf = aifunc.Conf(\n",
    "    model_deployment_name=\"gpt-5\",\n",
    "    reasoning_effort=\"medium\",\n",
    "    verbosity=\"low\",\n",
    "    temperature=None,  # reasoning models only support None, openai.NOT_GIVEN or default value of temperature\n",
    ")\n",
    "df[\"translation\"] = df[\"text\"].ai.translate(\"english\", conf=advanced_conf)\n",
    "```\n",
    "\n",
    "#### Option 2: Full control with `ai.generate_response`\n",
    "\n",
    "For maximum control, use `ai.generate_response` with a custom `response_format`:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class TranslationResult(BaseModel):\n",
    "    translation: str = Field(description=\"The translated text\")\n",
    "    formality: Literal[\"formal\", \"informal\"] = Field(\n",
    "        description=\"The formality level used in translation\"\n",
    "    )\n",
    "    source_language: str = Field(description=\"Detected source language\")\n",
    "    notes: str = Field(description=\"Any cultural adaptations or translation notes\")\n",
    "\n",
    "df[\"result\"] = df.ai.generate_response(\n",
    "    prompt=\"\"\"Translate this text to English: {text}\n",
    "    \n",
    "    Requirements:\n",
    "    - Use formal register\n",
    "    - Preserve technical terms in their original language with English explanation in parentheses\n",
    "    - Maintain the same paragraph structure\n",
    "    - Note any cultural adaptations made\"\"\",\n",
    "    is_prompt_template=True,\n",
    "    response_format=TranslationResult\n",
    ")\n",
    "```\n",
    "\n",
    "Use `Literal` for constrained choices and `Field(description=...)` to guide the model.\n",
    "\n",
    "---\n",
    "\n",
    "## Learn More\n",
    "\n",
    "- [ai.translate docs](https://learn.microsoft.com/fabric/data-science/ai-functions/pandas/translate)\n",
    "- [ai.generate_response docs](https://learn.microsoft.com/fabric/data-science/ai-functions/pandas/generate-response)\n",
    "- [Configuration options](https://learn.microsoft.com/fabric/data-science/ai-functions/pandas/configuration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
