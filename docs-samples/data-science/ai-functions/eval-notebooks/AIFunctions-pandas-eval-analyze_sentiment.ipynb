{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate ai.analyze_sentiment(...) Quality\n",
    "\n",
    "This notebook guides you through evaluating the output quality of AI Function `ai.analyze_sentiment` using **LLM-as-a-Judge** - a technique where a large language model acts as an evaluator to assess quality. In this setup, ground truth labels come from a larger judge model, not human labels. Use this starter notebook as a template: replace the sample data and adapt the evaluation prompts and criteria to your use case.\n",
    "\n",
    "### What You'll Do\n",
    "1. Run sentiment analysis on sample text data using `ai.analyze_sentiment` defaults\n",
    "2. Use a judge model to evaluate each prediction\n",
    "3. Calculate accuracy, precision, recall, and F1 score\n",
    "4. Identify which predictions need review\n",
    "5. *(Optional)* Refine with custom sentiment labels\n",
    "\n",
    "### Before You Start\n",
    "- **Other AI functions?** Find evaluation notebooks for all AI functions at [aka.ms/fabric-aifunctions-eval-notebooks](https://aka.ms/fabric-aifunctions-eval-notebooks)\n",
    "- **Runtime** - This notebook was made for **Fabric 1.3 runtime**.\n",
    "- **Customize this notebook** - The prompts and evaluation criteria below are a starting point. Adjust them to match your specific use case and quality standards.\n",
    "\n",
    "| Metric | Measures |\n",
    "|--------|----------|\n",
    "| **Accuracy** | Overall correctness |\n",
    "| **Precision** | Correct predictions per class |\n",
    "| **Recall** | Coverage per class |\n",
    "| **F1 Score** | Balance of precision & recall |\n",
    "\n",
    "[ai.analyze_sentiment pandas Documentation](https://learn.microsoft.com/fabric/data-science/ai-functions/pandas/analyze-sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "In Fabric 1.3 runtime, pandas AI functions require the openai-python package.\n",
    "\n",
    "See [install instructions for AI Functions](https://learn.microsoft.com/fabric/data-science/ai-functions/overview?tabs=pandas-pyspark%2Cpandas#install-dependencies) for up-to-date information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q openai 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synapse.ml.aifunc as aifunc\n",
    "from pydantic import BaseModel, Field\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# Executor: runs AI functions on your data\n",
    "executor_conf = aifunc.Conf(\n",
    "    model_deployment_name=\"gpt-4.1-mini\",  # see https://aka.ms/fabric-ai-models for other models\n",
    "    reasoning_effort=None,\n",
    "    verbosity=None,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# Judge: evaluates outputs (use a large frontier model with reasoning for best pseudo ground truth)\n",
    "judge_conf = aifunc.Conf(\n",
    "    model_deployment_name=\"gpt-5\",  # see https://aka.ms/fabric-ai-models for other models\n",
    "    reasoning_effort=\"low\",\n",
    "    verbosity=\"low\",\n",
    "    temperature=None,  # reasoning models only support None, openai.NOT_GIVEN or default value of temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Your Data\n",
    "\n",
    "Replace the sample data below with your own text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"\"\"I've used this laptop for a month and it's been great. The screen is sharp, \\\n",
    "the keyboard feels good, and the battery lasts a full workday.\"\"\",\n",
    "\n",
    "        \"\"\"My order arrived over a week late, and the device was cracked when I opened \\\n",
    "the box. Support kept me on hold and never followed up, so I'm very disappointed.\"\"\",\n",
    "\n",
    "        \"\"\"The strategy meeting had strong market insights, but the budget section ended \\\n",
    "without decisions. I left with useful notes and unresolved action items.\"\"\",\n",
    "\n",
    "        \"\"\"Dinner at the new Thai restaurant was delicious, especially the noodles and \\\n",
    "dessert. Service was slower than expected, but the staff were polite and helpful.\"\"\",\n",
    "\n",
    "        \"\"\"This project tool helps us plan sprints, but the mobile app crashes often and \\\n",
    "reports are hard to use. It saves time in some areas and creates extra work in others.\"\"\",\n",
    "\n",
    "        \"\"\"The city council approved the 2025 budget and published department allocations. \\\n",
    "Public works funding increased, while parks funding stayed the same.\"\"\",\n",
    "\n",
    "        \"\"\"The revised proposal moved the deadline up by two weeks even after the team \\\n",
    "raised capacity concerns. I am worried this timeline will lead to rushed work.\"\"\",\n",
    "\n",
    "        \"\"\"I came back from the data engineering summit energized. The keynote was \\\n",
    "practical, and I left with concrete ideas to try with my team.\"\"\"\n",
    "    ]\n",
    "})\n",
    "print(f\"Loaded {len(df)} samples\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Sentiment\n",
    "\n",
    "`ai.analyze_sentiment` works out of the box with default labels. Here we pass `executor_conf` so runs stay reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentiment with default labels using executor_conf\n",
    "df[\"sentiment\"] = df[\"text\"].ai.analyze_sentiment(conf=executor_conf)\n",
    "\n",
    "display(df[[\"text\", \"sentiment\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. (Optional) Custom Sentiment Labels\n",
    "\n",
    "By default, `ai.analyze_sentiment` uses: **positive, negative, neutral, mixed**.\n",
    "You can define custom labels for your specific use case.\n",
    "\n",
    "**Note:** This is a starter example. It may perform better or worse than baseline depending on your data. Test on your own data and tweak prompts, labels, schema, and model settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Emotion-based labels\n",
    "df[\"emotion\"] = df[\"text\"].ai.analyze_sentiment(\n",
    "    \"excited\", \"satisfied\", \"disappointed\", \"angry\", \"confused\",\n",
    "    conf=executor_conf,\n",
    ")\n",
    "\n",
    "# Example 2: Intensity-based labels\n",
    "df[\"intensity\"] = df[\"text\"].ai.analyze_sentiment(\n",
    "    \"very_positive\", \"positive\", \"neutral\", \"negative\", \"very_negative\",\n",
    "    conf=executor_conf,\n",
    ")\n",
    "\n",
    "# Example 3: Customer service specific\n",
    "df[\"urgency\"] = df[\"text\"].ai.analyze_sentiment(\n",
    "    \"urgent_complaint\", \"mild_concern\", \"neutral_inquiry\", \"positive_feedback\", \"enthusiastic_praise\",\n",
    "    conf=executor_conf,\n",
    ")\n",
    "\n",
    "# Compare default sentiment with custom label schemes\n",
    "\n",
    "display(df[[\"text\", \"sentiment\", \"emotion\", \"intensity\", \"urgency\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution\n",
    "colors = {\"positive\": \"#22cc77\", \"negative\": \"#ee4433\", \"neutral\": \"#999999\", \"mixed\": \"#ee7722\"}\n",
    "counts = df[\"sentiment\"].value_counts()\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.pie(counts, labels=counts.index, autopct=\"%1.0f%%\",\n",
    "       colors=[colors.get(x, \"#33aaaa\") for x in counts.index])\n",
    "\n",
    "ax.set_title(\"Sentiment Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Predictions\n",
    "\n",
    "A stronger model (`gpt-5`) judges whether each sentiment prediction is correct.\n",
    "\n",
    "> **TIP: XML-formatted prompts** - The evaluation prompt uses XML tags like `<evaluation_criteria>` and `<text>` to help LLMs distinguish between instructions and data. This improves accuracy. Try this pattern in your own prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"reason\" is first to encourage chain-of-thought reasoning before the LLM scores\n",
    "class SentimentEval(BaseModel):\n",
    "    reason: str = Field(description=\"Brief explanation for the evaluation decision\")\n",
    "    correct: bool = Field(description=\"Whether the predicted sentiment is correct\")\n",
    "    expected_sentiment: str = Field(description=\"The correct sentiment classification\")\n",
    "\n",
    "EVAL_PROMPT = \"\"\"You will evaluate whether a sentiment prediction is correct for a given text.\n",
    "<evaluation_criteria>\n",
    "Determine if the predicted sentiment accurately reflects the emotional tone of the text.\n",
    "- positive: The text expresses satisfaction, happiness, praise, or approval\n",
    "- negative: The text expresses dissatisfaction, frustration, criticism, or disapproval\n",
    "- neutral: The text is factual, objective, or lacks clear emotional tone\n",
    "- mixed: The text contains both positive and negative sentiments\n",
    "</evaluation_criteria>\n",
    "<text>\n",
    "{text}\n",
    "</text>\n",
    "<predicted_sentiment>\n",
    "{sentiment}\n",
    "</predicted_sentiment>\n",
    "Return whether the prediction is correct, and what the expected sentiment should be.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM-as-Judge Evaluation ---\n",
    "df[\"_eval_response\"] = df.ai.generate_response(\n",
    "    prompt=EVAL_PROMPT,\n",
    "    is_prompt_template=True,\n",
    "    conf=judge_conf,\n",
    "    response_format=SentimentEval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse structured JSON response\n",
    "df[\"correct\"] = df[\"_eval_response\"].apply(lambda x: json.loads(x)[\"correct\"])\n",
    "df[\"expected_sentiment\"] = df[\"_eval_response\"].apply(lambda x: json.loads(x)[\"expected_sentiment\"])\n",
    "df[\"eval_reason\"] = df[\"_eval_response\"].apply(lambda x: json.loads(x)[\"reason\"])\n",
    "\n",
    "display(df[[\"text\", \"sentiment\", \"expected_sentiment\", \"correct\", \"eval_reason\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Calculate metrics\n",
    "y_true = df[\"expected_sentiment\"]\n",
    "y_pred = df[\"sentiment\"]\n",
    "labels = sorted(set(y_true) | set(y_pred))\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "metrics_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
    "metrics_values = [accuracy, precision, recall, f1]\n",
    "metrics_df = pd.DataFrame({\"Metric\": metrics_names, \"Score\": metrics_values})\n",
    "\n",
    "display(metrics_df.round(3))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart for metrics\n",
    "bars = axes[0].bar(metrics_names, metrics_values, color=[\"#33aaaa\", \"#22cc77\", \"#9955bb\", \"#ee7722\"])\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "axes[0].axhline(y=0.9, color=\"#999999\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "for bar, val in zip(bars, metrics_values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, val + 0.02, f\"{val:.2f}\", ha=\"center\", fontweight=\"bold\")\n",
    "\n",
    "axes[0].set_title(\"Classification Metrics (Macro Average)\")\n",
    "\n",
    "# Pie chart for accuracy\n",
    "correct_count = df[\"correct\"].sum()\n",
    "total_count = len(df)\n",
    "pie_colors = [\"#22cc77\", \"#ee4433\"]\n",
    "pie_counts = [correct_count, total_count - correct_count]\n",
    "axes[1].pie(pie_counts, labels=[\"Correct\", \"Incorrect\"], autopct=\"%1.0f%%\", colors=pie_colors)\n",
    "axes[1].set_title(f\"Overall Accuracy: {accuracy*100:.0f}%\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df = pd.DataFrame({\"Metric\": metrics_names, \"Score\": metrics_values})\n",
    "report_df[\"Status\"] = report_df[\"Score\"].apply(\n",
    "    lambda x: \"Excellent\" if x >= 0.8 else \"Good\" if x >= 0.7 else \"Needs Work\"\n",
    ")\n",
    "overall_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"samples_evaluated\": len(df),\n",
    "            \"overall_accuracy\": round(accuracy, 3),\n",
    "            \"overall_status\": \"Excellent\" if accuracy >= 0.8 else \"Good\" if accuracy >= 0.7 else \"Needs Work\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(report_df.round(3))\n",
    "\n",
    "display(overall_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-sample breakdown\n",
    "breakdown = df[[\"text\", \"sentiment\", \"expected_sentiment\", \"correct\", \"eval_reason\"]].copy()\n",
    "breakdown[\"status\"] = breakdown[\"correct\"].apply(lambda x: \"PASS\" if x else \"FAIL\")\n",
    "breakdown[\"text\"] = breakdown[\"text\"].astype(str).str[:100] + \"...\"\n",
    "\n",
    "display(breakdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. (Optional) Refinement: Baseline vs Explainable Custom Sentiment\n",
    "\n",
    "Reuse the existing judge-derived `expected_sentiment` labels and compare baseline `ai.analyze_sentiment` with a custom `ai.generate_response` classifier that returns both a sentiment label and a short reason for explainability.\n",
    "\n",
    "**Note:** This is a starter example. It may perform better or worse than baseline depending on your data. Test on your own data and tweak prompts, labels, schema, and model settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class CustomSentimentPrediction(BaseModel):\n",
    "    reason: str = Field(description=\"Terse reason about which category should be chosen\")\n",
    "    sentiment: Literal[\"positive\", \"negative\", \"neutral\", \"mixed\"] = Field(\n",
    "        description=\"Sentiment label\"\n",
    "    )\n",
    "\n",
    "CUSTOM_SENTIMENT_PROMPT = \"\"\"Classify the sentiment of the text.\n",
    "<text>\n",
    "{text}\n",
    "</text>\n",
    "Think hard and reason about which sentiment is the best fit and why\n",
    "\"\"\"\n",
    "df[\"_custom_sentiment_response\"] = df.ai.generate_response(\n",
    "    prompt=CUSTOM_SENTIMENT_PROMPT,\n",
    "    is_prompt_template=True,\n",
    "    response_format=CustomSentimentPrediction,\n",
    "    conf=executor_conf\n",
    ")\n",
    "df[\"custom_sentiment\"] = df[\"_custom_sentiment_response\"].apply(lambda x: json.loads(x)[\"sentiment\"])\n",
    "df[\"custom_sentiment_reason\"] = df[\"_custom_sentiment_response\"].apply(lambda x: json.loads(x)[\"reason\"])\n",
    "\n",
    "display(df[[\"text\", \"sentiment\", \"expected_sentiment\", \"custom_sentiment\", \"custom_sentiment_reason\"]])\n",
    "\n",
    "y_true_compare = df[\"expected_sentiment\"]\n",
    "baseline_pred = df[\"sentiment\"]\n",
    "custom_pred = df[\"custom_sentiment\"]\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"],\n",
    "    \"Baseline\": [\n",
    "        accuracy_score(y_true_compare, baseline_pred),\n",
    "        precision_score(y_true_compare, baseline_pred, average=\"macro\", zero_division=0),\n",
    "        recall_score(y_true_compare, baseline_pred, average=\"macro\", zero_division=0),\n",
    "        f1_score(y_true_compare, baseline_pred, average=\"macro\", zero_division=0),\n",
    "    ],\n",
    "    \"Custom\": [\n",
    "        accuracy_score(y_true_compare, custom_pred),\n",
    "        precision_score(y_true_compare, custom_pred, average=\"macro\", zero_division=0),\n",
    "        recall_score(y_true_compare, custom_pred, average=\"macro\", zero_division=0),\n",
    "        f1_score(y_true_compare, custom_pred, average=\"macro\", zero_division=0),\n",
    "    ],\n",
    "})\n",
    "comparison_df[\"Delta\"] = comparison_df[\"Custom\"] - comparison_df[\"Baseline\"]\n",
    "\n",
    "display(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = comparison_df.set_index(\"Metric\")[[\"Baseline\", \"Custom\"]].plot(\n",
    "    kind=\"bar\", figsize=(7, 4), color=[\"#33aaaa\", \"#22cc77\"]\n",
    ")\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Baseline vs Custom Sentiment Metrics\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interpreting Results\n",
    "\n",
    "**Important:** These scores are LLM-judge proxies, not final ground truth.\n",
    "For fair comparisons, keep `judge_conf` fixed, change one `executor_conf` setting at a time, and confirm production decisions with human-reviewed samples.\n",
    "\n",
    "### Score Guide\n",
    "\n",
    "| Metric | Target | Meaning |\n",
    "|--------|--------|---------|\n",
    "| **Accuracy** | 90%+ | Overall correctness |\n",
    "| **Precision** | 90%+ | When it predicts X, is it right? |\n",
    "| **Recall** | 90%+ | Does it find all X's? |\n",
    "| **F1 Score** | 90%+ | Balance of precision & recall |\n",
    "\n",
    "| Score | Status |\n",
    "|-------|--------|\n",
    "| **80%+** | Excellent |\n",
    "| **70%-80%** | Good |\n",
    "| **<70%** | Needs Work |\n",
    "\n",
    "### Troubleshooting Low Scores\n",
    "\n",
    "| Issue | Likely Cause | Fix |\n",
    "|-------|--------------|-----|\n",
    "| Low accuracy | Ambiguous / sarcastic text | Provide custom labels that match your domain |\n",
    "| Low precision | False positives in a class | Add more granular labels (e.g. split \"mixed\") |\n",
    "| Low recall | Missed predictions for a class | Check if input text is too short or noisy |\n",
    "| Inconsistent results | Non-deterministic output | Set `temperature=0.0` in conf |\n",
    "\n",
    "---\n",
    "\n",
    "### Options for Improving Quality\n",
    "\n",
    "#### Option 1: Use custom sentiment labels\n",
    "\n",
    "```python\n",
    "# Instead of default (positive, negative, neutral, mixed)\n",
    "df[\"sentiment\"] = df[\"text\"].ai.analyze_sentiment(\n",
    "    \"very_positive\", \"positive\", \"neutral\", \"negative\", \"very_negative\",\n",
    "    conf=executor_conf,\n",
    ")\n",
    "```\n",
    "\n",
    "#### Option 2: Use a larger frontier reasoning model\n",
    "\n",
    "Larger frontier reasoning models have more cognitive horsepower and can improve quality on harder cases, with higher cost and latency.\n",
    "\n",
    "```python\n",
    "executor_conf = aifunc.Conf(model_deployment_name=\"gpt-4.1\")\n",
    "df[\"sentiment\"] = df[\"text\"].ai.analyze_sentiment(conf=executor_conf)\n",
    "\n",
    "# Or use gpt-5 reasoning for harder cases - more cognitive horsepower, higher cost/latency\n",
    "executor_conf = aifunc.Conf(\n",
    "    model_deployment_name=\"gpt-5\",\n",
    "    reasoning_effort=\"medium\",\n",
    "    verbosity=\"low\",\n",
    "    temperature=None,  # reasoning models only support None, openai.NOT_GIVEN or default value of temperature\n",
    ")\n",
    "df[\"sentiment\"] = df[\"text\"].ai.analyze_sentiment(conf=executor_conf)\n",
    "```\n",
    "\n",
    "#### Option 3: Build a custom sentiment function with `ai.generate_response`\n",
    "\n",
    "For maximum control, use `ai.generate_response` with a custom `response_format`. This approach can improve performance for domain-specific sentiment patterns:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class SentimentResult(BaseModel):\n",
    "    reason: str = Field(description=\"Brief explanation for the sentiment classification\")\n",
    "    sentiment: Literal[\"very positive\", \"positive\", \"neutral\", \"negative\", \"very negative\"] = Field(\n",
    "        description=\"The overall sentiment of the text\"\n",
    "    )\n",
    "    confidence: float = Field(description=\"Confidence score from 0 to 1\")\n",
    "\n",
    "df[\"result\"] = df.ai.generate_response(\n",
    "    prompt=\"\"\"Analyze the sentiment of this text: {text}\n",
    "    \n",
    "    Classify as: very positive, positive, neutral, negative, or very negative.\n",
    "    Provide your reasoning and a confidence score.\"\"\",\n",
    "    is_prompt_template=True,\n",
    "    conf=executor_conf,\n",
    "    response_format=SentimentResult\n",
    ")\n",
    "```\n",
    "\n",
    "Use `Literal` for constrained choices and `Field(description=...)` to guide the model.\n",
    "\n",
    "---\n",
    "\n",
    "## Learn More\n",
    "\n",
    "- [ai.analyze_sentiment docs](https://learn.microsoft.com/fabric/data-science/ai-functions/pandas/analyze-sentiment)\n",
    "- [ai.generate_response docs](https://learn.microsoft.com/fabric/data-science/ai-functions/pandas/generate-response)\n",
    "- [Configuration options](https://learn.microsoft.com/fabric/data-science/ai-functions/pandas/configuration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
